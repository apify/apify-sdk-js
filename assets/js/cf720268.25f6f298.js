"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[2987],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>c});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=u(n),c=o,m=d["".concat(s,".").concat(c)]||d[c]||h[c]||i;return n?a.createElement(m,r(r({ref:t},p),{},{components:n})):a.createElement(m,r({ref:t},p))}));function c(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var u=2;u<i;u++)r[u]=n[u];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},2865:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>c,frontMatter:()=>l,metadata:()=>u,toc:()=>h});var a=n(87462),o=n(63366),i=(n(67294),n(3905)),r=["components"],l={id_old:"version-1.3-getting-started",title:"Getting Started",id:"getting-started"},s=void 0,u={unversionedId:"guides/getting-started",id:"version-1.3/guides/getting-started",title:"Getting Started",description:"Without the right tools, crawling and scraping the web can be difficult. At the very least, you need an HTTP client to make the necessary",source:"@site/versioned_docs/version-1.3/guides/getting_started.md",sourceDirName:"guides",slug:"/guides/getting-started",permalink:"/sdk/js/docs/1.3/guides/getting-started",draft:!1,tags:[],version:"1.3",lastUpdatedBy:"Martin Ad\xe1mek",lastUpdatedAt:1675948676,formattedLastUpdatedAt:"Feb 9, 2023",frontMatter:{id_old:"version-1.3-getting-started",title:"Getting Started",id:"getting-started"},sidebar:"version-1.3/docs",previous:{title:"Apify Platform",permalink:"/sdk/js/docs/1.3/guides/apify-platform"},next:{title:"Request Storage",permalink:"/sdk/js/docs/1.3/guides/request-storage"}},p={},h=[{value:"Intro",id:"intro",level:2},{value:"Setting up locally",id:"setting-up-locally",level:2},{value:"Creating a new project",id:"creating-a-new-project",level:3},{value:"Setting up on the Apify Platform",id:"setting-up-on-the-apify-platform",level:2},{value:"Creating a new project",id:"creating-a-new-project-1",level:3},{value:"First crawler",id:"first-crawler",level:2},{value:"The general idea",id:"the-general-idea",level:3},{value:"The Where - <code>Request</code>, <code>RequestList</code> and <code>RequestQueue</code>",id:"the-where---request-requestlist-and-requestqueue",level:3},{value:"The What - <code>handlePageFunction</code>",id:"the-what---handlepagefunction",level:3},{value:"Putting it all together",id:"putting-it-all-together",level:3},{value:"CheerioCrawler aka jQuery crawler",id:"cheeriocrawler-aka-jquery-crawler",level:2},{value:"Overview",id:"overview",level:3},{value:"When to use <code>CheerioCrawler</code>",id:"when-to-use-cheeriocrawler",level:3},{value:"Basic use of <code>CheerioCrawler</code>",id:"basic-use-of-cheeriocrawler",level:3},{value:"Refresher",id:"refresher",level:4},{value:"Finding new links",id:"finding-new-links",level:4},{value:"Filtering links to same domain",id:"filtering-links-to-same-domain",level:4},{value:"Enqueueing links to <code>RequestQueue</code>",id:"enqueueing-links-to-requestqueue",level:4},{value:"Scrape the newly enqueued links",id:"scrape-the-newly-enqueued-links",level:4},{value:"The <code>maxRequestsPerCrawl</code> limit",id:"the-maxrequestspercrawl-limit",level:5},{value:"Putting it all together",id:"putting-it-all-together-1",level:4},{value:"Using Apify SDK to enqueue links like a boss",id:"using-apify-sdk-to-enqueue-links-like-a-boss",level:2},{value:"Meet <code>Apify.utils</code>",id:"meet-apifyutils",level:3},{value:"Introduction to <code>Apify.utils.enqueueLinks()</code>",id:"introduction-to-apifyutilsenqueuelinks",level:3},{value:"Basic use of <code>enqueueLinks()</code> with <code>CheerioCrawler</code>",id:"basic-use-of-enqueuelinks-with-cheeriocrawler",level:3},{value:"Introduction to pseudo-URLs",id:"introduction-to-pseudo-urls",level:4},{value:"Structure of a pseudo-URL",id:"structure-of-a-pseudo-url",level:4},{value:"Using <code>enqueueLinks()</code> to filter links",id:"using-enqueuelinks-to-filter-links",level:4},{value:"Resolving relative URLs with <code>enqueueLinks()</code>",id:"resolving-relative-urls-with-enqueuelinks",level:4},{value:"Integrating <code>enqueueLinks()</code> into our crawler",id:"integrating-enqueuelinks-into-our-crawler",level:4},{value:"Getting some real-world data",id:"getting-some-real-world-data",level:2},{value:"The importance of having a plan",id:"the-importance-of-having-a-plan",level:3},{value:"Choosing the data we need",id:"choosing-the-data-we-need",level:4},{value:"Analyzing the target",id:"analyzing-the-target",level:4},{value:"The start URL(s)",id:"the-start-urls",level:4},{value:"The crawling strategy",id:"the-crawling-strategy",level:3},{value:"Using a <code>RequestList</code>",id:"using-a-requestlist",level:4},{value:"DevTools crash course",id:"devtools-crash-course",level:4},{value:"Enqueueing the detail links using a custom selector",id:"enqueueing-the-detail-links-using-a-custom-selector",level:4},{value:"The <code>selector</code> parameter of <code>enqueueLinks()</code>",id:"the-selector-parameter-of-enqueuelinks",level:5},{value:"The missing <code>pseudoUrls</code>",id:"the-missing-pseudourls",level:5},{value:"Finally, the <code>userData</code> of <code>enqueueLinks()</code>",id:"finally-the-userdata-of-enqueuelinks",level:5},{value:"Another sanity check",id:"another-sanity-check",level:4},{value:"Scraping data",id:"scraping-data",level:3},{value:"Scraping the URL, Owner and Unique identifier",id:"scraping-the-url-owner-and-unique-identifier",level:4},{value:"Scraping Title, Description, Last modification date and Number of runs",id:"scraping-title-description-last-modification-date-and-number-of-runs",level:4},{value:"Title",id:"title",level:5},{value:"Description",id:"description",level:5},{value:"Last modification date",id:"last-modification-date",level:5},{value:"Run count",id:"run-count",level:5},{value:"Trying it out (sanity check #3)",id:"trying-it-out-sanity-check-3",level:4},{value:"Saving the scraped data",id:"saving-the-scraped-data",level:3},{value:"What&#39;s <code>Apify.pushData()</code>",id:"whats-apifypushdata",level:4},{value:"Finding my saved data",id:"finding-my-saved-data",level:4},{value:"Dataset on the Apify platform",id:"dataset-on-the-apify-platform",level:5},{value:"Local Dataset",id:"local-dataset",level:5},{value:"Final touch",id:"final-touch",level:3},{value:"Meet the <code>INPUT</code>",id:"meet-the-input",level:4},{value:"Use <code>INPUT</code> to seed our actor with categories",id:"use-input-to-seed-our-actor-with-categories",level:4},{value:"Structuring the code better",id:"structuring-the-code-better",level:4},{value:"Splitting your code into multiple files",id:"splitting-your-code-into-multiple-files",level:4},{value:"Using <code>Apify.utils.log</code> instead of <code>console.log</code>",id:"using-apifyutilslog-instead-of-consolelog",level:4},{value:"Using a router to structure your crawling",id:"using-a-router-to-structure-your-crawling",level:4}],d={toc:h};function c(e){var t=e.components,l=(0,o.Z)(e,r);return(0,i.kt)("wrapper",(0,a.Z)({},d,l,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Without the right tools, crawling and scraping the web can be difficult. At the very least, you need an HTTP client to make the necessary\nrequests, but that only gets you raw HTML and sometimes not even that. Then you have to read this HTML and extract the data you're interested in. Once\nextracted, it must be stored in a machine-readable format and easily accessible for further processing, because it is the processed data that holds\nvalue."),(0,i.kt)("p",null,"Apify SDK covers the process end-to-end. From crawling the web for links and scraping the raw data to storing it in various machine readable formats,\nready for processing. With this guide in hand, you should have your own data extraction solutions up and running in a few hours."),(0,i.kt)("h2",{id:"intro"},"Intro"),(0,i.kt)("p",null,"The goal of this getting started guide is to provide a step-by-step introduction to all the features of the Apify SDK. It will walk you through\ncreating the simplest of crawlers that only prints text to console, all the way up to complex systems that crawl pages, interact with them as if a real\nuser were sitting in front of a real browser and output structured data."),(0,i.kt)("p",null,"Since Apify SDK is usable both locally on any computer and on the ",(0,i.kt)("a",{parentName:"p",href:"../guides/apify-platform",target:null,rel:null},"Apify platform"),", you will be able\nto use the source code in both environments interchangeably. Nevertheless, some initial setup is still required, so choose your preferred starting\nenvironment and let's get into it."),(0,i.kt)("h2",{id:"setting-up-locally"},"Setting up locally"),(0,i.kt)("p",null,"To run Apify SDK on your own computer, you need to meet the following pre-requisites first:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Have Node.js version 10.17 or higher, with the exception of Node.js 11, installed.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Visit ",(0,i.kt)("a",{parentName:"li",href:"https://nodejs.org/en/download/",target:"_blank",rel:"noopener"},"Node.js website")," to download or use\n",(0,i.kt)("a",{parentName:"li",href:"https://github.com/creationix/nvm",target:"_blank",rel:"noopener"},"nvm")))),(0,i.kt)("li",{parentName:"ol"},"Have NPM installed.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"NPM comes bundled with Node.js so you should already have it. If not, reinstall Node.js.")))),(0,i.kt)("p",null,"If you're not certain, confirm the prerequisites by running:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"node -v\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"npm -v\n")),(0,i.kt)("h3",{id:"creating-a-new-project"},"Creating a new project"),(0,i.kt)("p",null,"The fastest and best way to create new projects with the Apify SDK is to use our own\n",(0,i.kt)("a",{parentName:"p",href:"https://www.npmjs.com/package/apify-cli",target:"_blank",rel:"noopener"},"Apify CLI"),". This command line tool allows you to create, run and manage Apify\nprojects with ease, including their deployment to the ",(0,i.kt)("a",{parentName:"p",href:"../guides/apify-platform",target:null,rel:null},"Apify platform")," if you wish to run them in the\ncloud after developing them locally."),(0,i.kt)("p",null,"Let's install the Apify CLI with the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"npm install -g apify-cli\n")),(0,i.kt)("p",null,"Once the installation finishes, all you need to do to set up an Apify SDK project is to run:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"apify create my-new-project\n")),(0,i.kt)("p",null,"A prompt will be shown, asking you to choose a template. Disregard the different options for now and choose the template labeled ",(0,i.kt)("inlineCode",{parentName:"p"},"Hello world"),". The\ncommand will now create a new directory in your current working directory, called ",(0,i.kt)("inlineCode",{parentName:"p"},"my-new-project"),", create a ",(0,i.kt)("inlineCode",{parentName:"p"},"package.json")," in this folder and install\nall the necessary dependencies. It will also add example source code that you can immediately run."),(0,i.kt)("p",null,"Let's try that!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"cd my-new-project\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"apify run -p\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"The ",(0,i.kt)("inlineCode",{parentName:"p"},"-p")," flag is great to remember, because it stands for ",(0,i.kt)("inlineCode",{parentName:"p"},"--purge")," and it clears out your persistent storages before starting the actor.\n",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT.json")," and named storages are kept. Whenever you're just restarting your actor and you're not interested in the data of the previous run, you\nshould use ",(0,i.kt)("inlineCode",{parentName:"p"},"apify run -p")," to prevent the old state from messing with your current run. If this is confusing, don't worry. You'll learn about\nstorages and ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT.json")," soon.")),(0,i.kt)("p",null,"You should start seeing log messages in the terminal as the system boots up and after a second, a Chromium browser window should pop up. In the\nwindow, you'll see quickly changing pages and back in the terminal, you should see the titles (contents of the ",(0,i.kt)("inlineCode",{parentName:"p"},"<title>")," HTML tags) of the pages\nprinted."),(0,i.kt)("p",null,"You can always terminate the crawl with a keypress in the terminal:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"CTRL+C\n")),(0,i.kt)("p",null,"Did you see all that? If you did, congratulations! You're ready to go!"),(0,i.kt)("h2",{id:"setting-up-on-the-apify-platform"},"Setting up on the Apify Platform"),(0,i.kt)("p",null,"Maybe you don't have Node.js installed and don't want the hassle. Or you can't install anything on your computer because you're using one provided by your company. Or perhaps you'd just prefer to start working in the cloud right away. Well, no worries, we've got you covered."),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"../guides/apify-platform",target:null,rel:null},"Apify platform")," is the foundational product of\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com",target:"_blank",rel:"noopener"},"Apify"),". It's a serverless cloud computing platform, specifically designed for any web automation jobs,\nthat may include crawling and scraping, but really works amazingly for any batch jobs and long-running tasks."),(0,i.kt)("p",null,"It comes with a free account, so let's go to our ",(0,i.kt)("a",{parentName:"p",href:"https://my.apify.com/sign-up",target:"_blank",rel:"noopener"},"sign-up page")," and create one, if you\nhaven't already. Don't forget to verify your email. Without it, you won't be able to run any projects."),(0,i.kt)("p",null,"Once you're in, you might be prompted by our in-app help to walk through a step-by-step guide to some of our new features. Feel free to finish that,\nif you'd like, but once you're done, click on the ",(0,i.kt)("strong",{parentName:"p"},"Actors")," tab in the left menu. To read more about ",(0,i.kt)("strong",{parentName:"p"},"Actors"),", see:\n",(0,i.kt)("a",{parentName:"p",href:"../guides/apify-platform#what-is-an-actor",target:null,rel:null},"What is an actor")),(0,i.kt)("h3",{id:"creating-a-new-project-1"},"Creating a new project"),(0,i.kt)("p",null,"In the page that shows after clicking on Actors in the left menu, choose ",(0,i.kt)("strong",{parentName:"p"},"Create new"),". Give it a name in the form that opens, let's say,\n",(0,i.kt)("inlineCode",{parentName:"p"},"my-new-actor"),". Disregard all the available options for now and save your changes."),(0,i.kt)("p",null,"Now click on the ",(0,i.kt)("strong",{parentName:"p"},"Sources")," tab at the top. Disregard the version and environment variables inputs for now and proceed directly to ",(0,i.kt)("strong",{parentName:"p"},"Source code"),".\nThis is where you develop the actor, if you choose not to do it locally. Just press ",(0,i.kt)("strong",{parentName:"p"},"Run")," below the ",(0,i.kt)("strong",{parentName:"p"},"Source code")," panel. It will automatically\nbuild and run the example source code. You should start seeing log messages that represent the build and after the build is complete, the log messages of\nthe running actor. Feel free to check out the other ",(0,i.kt)("strong",{parentName:"p"},"Run")," tabs, such as ",(0,i.kt)("strong",{parentName:"p"},"Info"),", where you can find useful information about the run, or\n",(0,i.kt)("strong",{parentName:"p"},"Key-value-store"),", where the actor's ",(0,i.kt)("strong",{parentName:"p"},"INPUT"),"\xa0and ",(0,i.kt)("strong",{parentName:"p"},"OUTPUT")," are stored."),(0,i.kt)("p",null,"Good job. You're now ready to run your own source code on the Apify Platform. For more information, visit the\n",(0,i.kt)("a",{parentName:"p",href:"https://docs.apify.com/actor",target:"_blank",rel:"noopener"},"Actor documentation page"),", where you'll find everything about the platform's various\noptions."),(0,i.kt)("h2",{id:"first-crawler"},"First crawler"),(0,i.kt)("p",null,"Whether you've chosen to develop locally or in the cloud, it's time to start writing some actual source code. But before we do, let's just briefly\nintroduce all the Apify SDK classes necessary to make it happen."),(0,i.kt)("h3",{id:"the-general-idea"},"The general idea"),(0,i.kt)("p",null,"There are 3 crawler classes available for use in the Apify SDK. ",(0,i.kt)("a",{parentName:"p",href:"../api/basic-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"BasicCrawler")),", ",(0,i.kt)("a",{parentName:"p",href:"../api/cheerio-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"CheerioCrawler")),"\nand ",(0,i.kt)("a",{parentName:"p",href:"../api/puppeteer-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"PuppeteerCrawler")),". We'll talk about their differences later. Now, let's talk about what they have in common."),(0,i.kt)("p",null,"The general idea of each crawler is to go to a web page, open it, do some stuff there, save some results and continue to the next page, until it's done\nits job. So the crawler always needs to find answers to two questions: ",(0,i.kt)("strong",{parentName:"p"},"Where should I go?")," and ",(0,i.kt)("strong",{parentName:"p"},"What should I do there?")," Answering those two\nquestions is the only setup mandatory for running the crawlers."),(0,i.kt)("h3",{id:"the-where---request-requestlist-and-requestqueue"},"The Where - ",(0,i.kt)("inlineCode",{parentName:"h3"},"Request"),", ",(0,i.kt)("inlineCode",{parentName:"h3"},"RequestList")," and ",(0,i.kt)("inlineCode",{parentName:"h3"},"RequestQueue")),(0,i.kt)("p",null,"All crawlers use instances of the ",(0,i.kt)("a",{parentName:"p",href:"../api/request",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"Request"))," class to determine where they need to go. Each request may hold a lot of information,\nbut at the very least, it must hold a URL - a web page to open. But having only one URL would not make sense for crawling. We need to either have a\npre-existing list of our own URLs that we wish to visit, perhaps a thousand, or a million, or we need to build this list dynamically as we crawl,\nadding more and more URLs to the list as we progress."),(0,i.kt)("p",null,"A representation of the pre-existing list is an instance of the ",(0,i.kt)("a",{parentName:"p",href:"../api/request-list",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"RequestList"))," class. It is a static, immutable list of URLs and\nother metadata (see the ",(0,i.kt)("a",{parentName:"p",href:"../api/request",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"Request"))," object) that the crawler will visit, one by one, retrying whenever an error occurs, until there\nare no more ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," to process."),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"../api/request-queue",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"RequestQueue"))," on the other hand, represents a dynamic queue of ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests"),". One that can be updated at runtime by adding more\npages - ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," to process. This allows the crawler to open one page, extract interesting URLs, such as links to other pages on the same domain,\nadd them to the queue (called ",(0,i.kt)("em",{parentName:"p"},"enqueuing"),") and repeat this process to build a queue of tens of thousands or more URLs while knowing only a single one\nat the beginning."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," are essential for the crawler's operation. There is no other way to supply ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests"),' = "pages to crawl" to the\ncrawlers. At least one of them always needs to be provided while setting up. You can also use both at the same time, if you wish.'),(0,i.kt)("h3",{id:"the-what---handlepagefunction"},"The What - ",(0,i.kt)("inlineCode",{parentName:"h3"},"handlePageFunction")),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," is the brain of the crawler. It tells it what to do at each and every page it visits. Generally it handles extraction of data\nfrom the page, processing the data, saving it, calling APIs, doing calculations and whatever else you need it to do."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," is provided by you, the user, and invoked automatically by the crawler for each ",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," from either the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," or\n",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue"),". It always receives a single argument and that is a plain ",(0,i.kt)("inlineCode",{parentName:"p"},"Object"),". Its properties change depending on the crawler class used, but it\nalways includes at least the ",(0,i.kt)("inlineCode",{parentName:"p"},"request")," property, which represents the currently crawled ",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," instance (i.e. the URL the crawler is visiting and\nrelated metadata) and the ",(0,i.kt)("inlineCode",{parentName:"p"},"autoscaledPool")," property, which is an instance of the ",(0,i.kt)("a",{parentName:"p",href:"../api/autoscaled-pool",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"AutoscaledPool"))," class and we'll talk about\nit in detail later."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// The object received as a single argument by the handlePageFunction\n{\n    request: Request,\n    autoscaledPool: AutoscaledPool\n}\n")),(0,i.kt)("h3",{id:"putting-it-all-together"},"Putting it all together"),(0,i.kt)("p",null,"Enough theory! Let's put some of those hard-learned facts into practice. We learned above that we need ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," and a ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," to setup\na crawler. We will also use the ",(0,i.kt)("a",{parentName:"p",href:"../api/apify#main",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"Apify.main()"))," function. It's not mandatory, but it makes our life easier. We'll\nlearn about it in detail later on."),(0,i.kt)("p",null,"Let's start with something super easy. Visit a page, get its title and close. First of all we need to require Apify, to make all of its features available to us:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n")),(0,i.kt)("p",null,"Easy, right? It really doesn't get much more difficult than that. For the purposes of this tutorial, we'll be scraping our own webpage\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com",target:"_blank",rel:"noopener"},"https://apify.com"),". Now, to get there, we need a ",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," with the page's URL in one of our sources,\n",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue"),". Let's go with ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," for now."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n\n// This is how you use the Apify.main() function.\nApify.main(async () => {\n    // First we create the request queue instance.\n    const requestQueue = await Apify.openRequestQueue();\n    // And then we add a request to it.\n    await requestQueue.addRequest({ url: 'https://apify.com' });\n});\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"If you're not familiar with the ",(0,i.kt)("inlineCode",{parentName:"p"},"async")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"await")," keywords used in the example, you should know that these are native syntax in modern JavaScript. You can\n",(0,i.kt)("a",{parentName:"p",href:"https://nikgrozev.com/2017/10/01/async-await/",target:"_blank",rel:"noopener"},"learn more about them here"),".")),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"../api/request-queue#addrequest",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"requestQueue.addRequest()"))," function automatically converts the plain object we passed to it to a\n",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," instance, so now we have a ",(0,i.kt)("inlineCode",{parentName:"p"},"requestQueue")," that holds one ",(0,i.kt)("inlineCode",{parentName:"p"},"request")," which points to ",(0,i.kt)("inlineCode",{parentName:"p"},"https://apify.com"),". Now we need the\n",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// We'll define the function separately so it's more obvious.\nconst handlePageFunction = async ({ request, $ }) => {\n    // This should look familiar if you ever worked with jQuery.\n    // We're just getting the text content of the <title> HTML element.\n    const title = $('title').text();\n\n    console.log(`The title of \"${request.url}\" is: ${title}.`);\n};\n")),(0,i.kt)("p",null,"Wait, where did the ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," come from? Remember what we learned about the ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," earlier. It expects a plain ",(0,i.kt)("inlineCode",{parentName:"p"},"Object")," as an argument that\nwill always have a ",(0,i.kt)("inlineCode",{parentName:"p"},"request")," property, but it will also have other properties, depending on the chosen crawler class. Well, ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," is a property provided\nby the ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler")," class, which we'll set up right now."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n\nApify.main(async () => {\n    const requestQueue = await Apify.openRequestQueue();\n    await requestQueue.addRequest({ url: 'https://apify.com' });\n\n    const handlePageFunction = async ({ request, $ }) => {\n        const title = $('title').text();\n\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    };\n\n    // Set up the crawler, passing a single options object as an argument.\n    const crawler = new Apify.CheerioCrawler({\n        requestQueue,\n        handlePageFunction,\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("p",null,"And we're done! You just created your first crawler from scratch. It will download the HTML of ",(0,i.kt)("inlineCode",{parentName:"p"},"https://apify.com"),", find the ",(0,i.kt)("inlineCode",{parentName:"p"},"<title>")," element, get\nits text content and print it to console. Good job!"),(0,i.kt)("p",null,"To run the code locally, copy and paste the code, if you haven't already typed it in yourself, to the ",(0,i.kt)("inlineCode",{parentName:"p"},"main.js")," file in the ",(0,i.kt)("inlineCode",{parentName:"p"},"my-new-project")," we\ncreated earlier and run ",(0,i.kt)("inlineCode",{parentName:"p"},"apify run")," from that project's directory."),(0,i.kt)("p",null,"To run the code on Apify Platform, just replace the original example with your new code and hit Run."),(0,i.kt)("p",null,"Whichever environment you choose, you should see the message\n",(0,i.kt)("inlineCode",{parentName:"p"},'The title of "https://apify.com" is: Web Scraping, Data Extraction and Automation - Apify.')," printed to the screen. If you do, congratulations and\nlet's move onto some bigger challenges! And if you feel like you don't really know what just happened there, no worries, it will all become clear when\nyou learn more about ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),"."),(0,i.kt)("h2",{id:"cheeriocrawler-aka-jquery-crawler"},"CheerioCrawler aka jQuery crawler"),(0,i.kt)("p",null,"This is the crawler that we used in our earlier example. Our simplest and also the fastest crawling solution. If you're familiar with ",(0,i.kt)("inlineCode",{parentName:"p"},"jQuery"),", you'll\nunderstand ",(0,i.kt)("a",{parentName:"p",href:"../api/cheerio-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"CheerioCrawler"))," in minutes. ",(0,i.kt)("a",{parentName:"p",href:"https://www.npmjs.com/package/cheerio",target:"_blank",rel:"noopener"},"Cheerio")," is\nessentially ",(0,i.kt)("inlineCode",{parentName:"p"},"jQuery")," for Node.js. It offers the same API, including the familiar ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," object. You can use it, as you would ",(0,i.kt)("inlineCode",{parentName:"p"},"jQuery"),", for manipulating\nthe DOM of a HTML page. In crawling, you'll mostly use it to select the right elements and extract their text values - the data you're interested in.\nBut ",(0,i.kt)("inlineCode",{parentName:"p"},"jQuery")," runs in a browser and attaches directly to the browser's DOM. Where does ",(0,i.kt)("inlineCode",{parentName:"p"},"cheerio")," get its HTML? This is where the ",(0,i.kt)("inlineCode",{parentName:"p"},"Crawler")," part of\n",(0,i.kt)("a",{parentName:"p",href:"../api/cheerio-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"CheerioCrawler"))," comes in."),(0,i.kt)("h3",{id:"overview"},"Overview"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"../api/cheerio-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"CheerioCrawler"))," crawls by making plain HTTP requests to the provided URLs. As you remember from the previous section, the\nURLs are fed to the crawler using either the ",(0,i.kt)("a",{parentName:"p",href:"../api/cheerio-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"RequestList"))," or the ",(0,i.kt)("a",{parentName:"p",href:"../api/cheerio-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"RequestQueue")),". The HTTP responses\nit gets back are HTML pages, the same pages you would get in your browser when you first load a URL."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Note, however, that modern web pages often do not serve all of their content in the first HTML response, but rather the first HTML contains links to\nother resources such as CSS and JavaScript that get downloaded afterwards and together they create the final page. See our\n",(0,i.kt)("a",{parentName:"p",href:"../api/puppeteer-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"PuppeteerCrawler"))," to crawl those.")),(0,i.kt)("p",null,"Once the page's HTML is retrieved, the crawler will pass it to ",(0,i.kt)("a",{parentName:"p",href:"https://www.npmjs.com/package/cheerio",target:"_blank",rel:"noopener"},"Cheerio")," for\nparsing. The result is the typical ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," function, which should be familiar to ",(0,i.kt)("inlineCode",{parentName:"p"},"jQuery")," users. You can use this ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," to do all sorts of lookups and\nmanipulation of the page's HTML, but in scraping, we will mostly use it to find specific HTML elements and extract their data."),(0,i.kt)("p",null,"Example use of Cheerio and its ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," function in comparison to browser JavaScript:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"This is not to show that Cheerio is better than plain browser JavaScript. Some might actually prefer the more expressive way plain JS provides.\nUnfortunately, the browser JavaScript methods are not available in Node.js, so Cheerio is our best bet to do the parsing.")),(0,i.kt)("h3",{id:"when-to-use-cheeriocrawler"},"When to use ",(0,i.kt)("inlineCode",{parentName:"h3"},"CheerioCrawler")),(0,i.kt)("p",null,"Even though using ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler")," is extremely easy, it probably will not be your first choice for most kinds of crawling or scraping in production\nenvironments. Since most websites nowadays use modern JavaScript to create rich, responsive and data-driven user experiences, the plain HTTP requests\nthe crawler uses may just fall short of your needs."),(0,i.kt)("p",null,"But ",(0,i.kt)("a",{parentName:"p",href:"../api/cheerio-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"CheerioCrawler"))," is far from useless! It really shines when you need to cope with extremely high workloads. With just 4 GBs of memory and a single CPU\ncore, you can scrape 500 or more pages a minute! ",(0,i.kt)("em",{parentName:"p"},"(assuming each page contains approximately 400KB of HTML)")," To scrape this fast with a full browser\nscraper, such as the ",(0,i.kt)("a",{parentName:"p",href:"../api/puppeteer-crawler",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"PuppeteerCrawler")),", you'd need significantly more computing power."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Advantages:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Extremely fast"),(0,i.kt)("li",{parentName:"ul"},"Easy to set up"),(0,i.kt)("li",{parentName:"ul"},"Familiar for jQuery users"),(0,i.kt)("li",{parentName:"ul"},"Super cheap to run"),(0,i.kt)("li",{parentName:"ul"},"Each request can go through a different proxy")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Disadvantages:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Does not work for all websites"),(0,i.kt)("li",{parentName:"ul"},"May easily overload the target website with requests"),(0,i.kt)("li",{parentName:"ul"},"Does not enable any manipulation of the website before scraping")),(0,i.kt)("h3",{id:"basic-use-of-cheeriocrawler"},"Basic use of ",(0,i.kt)("inlineCode",{parentName:"h3"},"CheerioCrawler")),(0,i.kt)("p",null,"Now that we have an idea of the crawler's inner workings, let's build one. We'll use the example from the previous section and improve on it by\nletting it truly crawl the page, finding new links as it goes, enqueuing them into the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," and then scraping them."),(0,i.kt)("h4",{id:"refresher"},"Refresher"),(0,i.kt)("p",null,"Just to refresh your memory, in the previous section we built a very simple crawler that downloads the HTML of a single page, reads its title and prints\nit to the console. This is the original source code:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n\nApify.main(async () => {\n    const requestQueue = await Apify.openRequestQueue();\n    await requestQueue.addRequest({ url: 'https://apify.com' });\n\n    const handlePageFunction = async ({ request, $ }) => {\n        const title = $('title').text();\n\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    };\n\n    // Set up the crawler, passing a single options object as an argument.\n    const crawler = new Apify.CheerioCrawler({\n        requestQueue,\n        handlePageFunction,\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("p",null,"Earlier we said that we would let the crawler:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Find new links on the page"),(0,i.kt)("li",{parentName:"ol"},"Filter only those pointing to ",(0,i.kt)("inlineCode",{parentName:"li"},"apify.com")),(0,i.kt)("li",{parentName:"ol"},"Enqueue them to the ",(0,i.kt)("inlineCode",{parentName:"li"},"RequestQueue")),(0,i.kt)("li",{parentName:"ol"},"Scrape the newly enqueued links")),(0,i.kt)("p",null,"So let's get to it!"),(0,i.kt)("h4",{id:"finding-new-links"},"Finding new links"),(0,i.kt)("p",null,"There are numerous approaches to finding links to follow when crawling the web. For our purposes, we will be looking for ",(0,i.kt)("inlineCode",{parentName:"p"},"<a>")," elements that contain\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"href")," attribute. For example ",(0,i.kt)("inlineCode",{parentName:"p"},'<a href="https://apify.com/store>This is a link to Apify Store</a>'),". To do this, we need to update our Cheerio\nfunction."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const links = $('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n")),(0,i.kt)("p",null,"Our new function finds all the ",(0,i.kt)("inlineCode",{parentName:"p"},"<a>")," elements that contain the ",(0,i.kt)("inlineCode",{parentName:"p"},"href")," attribute and extracts the attributes into an array of strings. But there's a problem. There could be relative links in the list and those can't be used on their own. We need to resolve them using our domain as base URL and\nwe will use one of Node.js' standard libraries to do this."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// At the top of the file:\nconst { URL } = require('url');\n\n// ...\n\nconst ourDomain = 'https://apify.com';\nconst absoluteUrls = links.map(link => new URL(link, ourDomain));\n")),(0,i.kt)("h4",{id:"filtering-links-to-same-domain"},"Filtering links to same domain"),(0,i.kt)("p",null,"Websites typically contain a lot of links that lead away from the original page. This is normal, but when crawling a website, we usually want to crawl\nthat one site and not let our crawler wander away to Google, Facebook and Twitter. Therefore, we need to filter out the off-domain links and only\nkeep the ones that lead to the same domain."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Don't worry, we'll learn how to do this with a single function call using Apify in a few moments.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// At the top of the file:\nconst { URL } = require('url');\n\n// ...\n\nconst links = $('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n\nconst ourDomain = 'apify.com';\nconst absoluteUrls = links.map(link => new URL(link, ourDomain));\n\nconst sameDomainLinks = absoluteUrls.filter(url => url.href.startsWith(ourDomain));\n\n// ...\n")),(0,i.kt)("h4",{id:"enqueueing-links-to-requestqueue"},"Enqueueing links to ",(0,i.kt)("inlineCode",{parentName:"h4"},"RequestQueue")),(0,i.kt)("p",null,"This should be easy, because we already did that ",(0,i.kt)("a",{parentName:"p",href:"#putting-it-all-together",target:null,rel:null},"earlier"),", remember? Just call ",(0,i.kt)("inlineCode",{parentName:"p"},"requestQueue.addRequest()")," for all the new\nlinks. This will add them to the end of the queue for processing."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// At the top of the file:\nconst { URL } = require('url');\n\n// ...\n\nconst links = $('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n\nconst ourDomain = 'https://apify.com';\nconst absoluteUrls = links.map(link => new URL(link, ourDomain));\n\nconst sameDomainLinks = absoluteUrls.filter(url => url.href.startsWith(ourDomain));\n\n// Add the requests in series. There's of course room for speed\n// improvement by parallelization. Try to implement it, if you wish.\nconsole.log(`Enqueueing ${sameDomainLinks.length} URLs.`);\nfor (const url of sameDomainLinks) {\n    await requestQueue.addRequest({ url: url.href });\n}\n\n// ...\n")),(0,i.kt)("h4",{id:"scrape-the-newly-enqueued-links"},"Scrape the newly enqueued links"),(0,i.kt)("p",null,"And we're approching the finishing line. All we need to do now is integrate the new code into our original crawler. It will be easy, because\nalmost everything needs to go into the ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction"),". But just before we do that, let's introduce the first crawler configuration option that\nis not a ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"requestQueue"),". It's called ",(0,i.kt)("inlineCode",{parentName:"p"},"maxRequestsPerCrawl"),"."),(0,i.kt)("h5",{id:"the-maxrequestspercrawl-limit"},"The ",(0,i.kt)("inlineCode",{parentName:"h5"},"maxRequestsPerCrawl")," limit"),(0,i.kt)("p",null,"This configuration option is available in all crawler classes and you can use it to limit the number of ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," the crawler should process. It's\nvery useful when you're just testing your code or when your crawler could potentially crawl millions of pages and you want to save resources. You can\nadd it to the crawler options like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const crawler = new Apify.CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    requestQueue,\n    handlePageFunction,\n});\n")),(0,i.kt)("p",null,"This limits the number of successfully handled ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," to 20. Bear in mind that the actual number of processed requests might be a little higher\nand that's because usually there are multiple ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," processed at the same time and once the 20th ",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," finishes, the other running ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests"),"\nwill be allowed to finish too."),(0,i.kt)("h4",{id:"putting-it-all-together-1"},"Putting it all together"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const { URL } = require('url'); // <------ This is new.\nconst Apify = require('apify');\n\nApify.main(async () => {\n    const requestQueue = await Apify.openRequestQueue();\n    await requestQueue.addRequest({ url: 'https://apify.com' });\n\n    const handlePageFunction = async ({ request, $ }) => {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // Here starts the new part of handlePageFunction.\n        const links = $('a[href]')\n            .map((i, el) => $(el).attr('href'))\n            .get();\n\n        const ourDomain = 'https://apify.com';\n        const absoluteUrls = links.map(link => new URL(link, ourDomain));\n\n        const sameDomainLinks = absoluteUrls.filter(url => url.href.startsWith(ourDomain));\n\n        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);\n        for (const url of sameDomainLinks) {\n            await requestQueue.addRequest({ url: url.href });\n        }\n    };\n\n    const crawler = new Apify.CheerioCrawler({\n        maxRequestsPerCrawl: 20, // <------ This is new too.\n        requestQueue,\n        handlePageFunction,\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("p",null,"No matter if you followed along with our coding or just copy-pasted the resulting source, try running it now, perhaps even in both environments. You\nshould see the crawler log the ",(0,i.kt)("strong",{parentName:"p"},"title")," of the first page, then the ",(0,i.kt)("strong",{parentName:"p"},"enqueueing")," message showing number of URLs, followed by the ",(0,i.kt)("strong",{parentName:"p"},"title")," of the\nfirst enqueued page and so on and so on."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"If you need help with running the code, refer back to the chapters on environment setup: ",(0,i.kt)("a",{parentName:"p",href:"#setting-up-locally",target:null,rel:null},"Setting up locally")," and\n",(0,i.kt)("a",{parentName:"p",href:"#setting-up-on-the-apify-platform",target:null,rel:null},"Setting up on the Apify platform"),".")),(0,i.kt)("h2",{id:"using-apify-sdk-to-enqueue-links-like-a-boss"},"Using Apify SDK to enqueue links like a boss"),(0,i.kt)("p",null,"If you were paying attention carefully in the previous chapter, we said that we would show you a way to enqueue new ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," with a single function\ncall. You might be wondering why we had to go through the whole process of getting the individual links, filtering the same domain ones and then\nmanually enqueuing them into the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue"),", when there is a simpler way."),(0,i.kt)("p",null,"Well, the obvious reason is practice. This is a tutorial after all. The other reason is to make you think about all the bits and pieces that come\ntogether, so that in the end, a new page, not previously entered in by you, can be scraped. We think that by seeing the bigger picture, you will be\nable to get the most out of Apify SDK."),(0,i.kt)("h3",{id:"meet-apifyutils"},"Meet ",(0,i.kt)("inlineCode",{parentName:"h3"},"Apify.utils")),(0,i.kt)("p",null,"We will talk at length about them later, but in short, ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.utils")," is a namespace where you can find various helpful functions and constants that\nmake your life easier. One of the available functions is ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.utils.enqueueLinks()")," which encapsulates the whole enqueueing process and even adds\nsome extra functionality."),(0,i.kt)("h3",{id:"introduction-to-apifyutilsenqueuelinks"},"Introduction to ",(0,i.kt)("inlineCode",{parentName:"h3"},"Apify.utils.enqueueLinks()")),(0,i.kt)("p",null,"Since enqueuing new links to crawl is such an integral part of web crawling, we created a function that attempts to simplify this process as much as\npossible. With a single function call, it allows you to find all the links on a page that match specified criteria and add them to a ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue"),".\nIt also allows you to modify the resulting ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," to match your crawling needs."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks")," is quite a powerful function so, like crawlers, it gets its arguments from an options object. This is useful, because you don't have to\nremember their order! But also because we can easily extend its API and add new features. You can\n",(0,i.kt)("a",{parentName:"p",href:"../api/utils#enqueuelinks",target:null,rel:null},"find the full reference here"),"."),(0,i.kt)("p",null,"We suggest using ES6 destructuring to grab the ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," function off of the ",(0,i.kt)("inlineCode",{parentName:"p"},"utils")," object, so you don't have to type ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.utils")," all the\ntime."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\nconst {\n    utils: { enqueueLinks },\n} = Apify;\n\n// Now you can use enqueueLinks like this:\nawait enqueueLinks({\n    /* options */\n});\n")),(0,i.kt)("h3",{id:"basic-use-of-enqueuelinks-with-cheeriocrawler"},"Basic use of ",(0,i.kt)("inlineCode",{parentName:"h3"},"enqueueLinks()")," with ",(0,i.kt)("inlineCode",{parentName:"h3"},"CheerioCrawler")),(0,i.kt)("p",null,"We already implemented logic that takes care of enqueueing new links to a ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," in the previous chapter on ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),". Let's look at\nthat logic and implement the same functionality using ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()"),"."),(0,i.kt)("p",null,"We found that the crawler needed to do these 4 things to crawl ",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com"),":"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Find new links on the page"),(0,i.kt)("li",{parentName:"ol"},"Filter only those pointing to ",(0,i.kt)("inlineCode",{parentName:"li"},"apify.com")),(0,i.kt)("li",{parentName:"ol"},"Enqueue them to the ",(0,i.kt)("inlineCode",{parentName:"li"},"RequestQueue")),(0,i.kt)("li",{parentName:"ol"},"Scrape the newly enqueued links")),(0,i.kt)("p",null,"Using ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," we can squash the first 3 into a single function call, if we set the options correctly. For now, let's just stick to the\nbasics. At the very least, we need a source where to find the links and the queue to enqueue them to. The ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," Cheerio object is one of the sources the\nfunction accepts and we already know how to work with it in the ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction"),". We also know how to get a ",(0,i.kt)("inlineCode",{parentName:"p"},"requestQueue")," instance."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// Assuming previous existence of the '$' and 'requestQueue' variables.\nawait enqueueLinks({ $, requestQueue });\n")),(0,i.kt)("p",null,"That's all we need to do to enqueue all ",(0,i.kt)("inlineCode",{parentName:"p"},'<a href="...">')," links from the given page to the given queue. Easy, right? Scratch number 1 and 3 off the\nlist. Only number 2 remains and to tackle this one, we need to talk about yet another new concept, the pseudo-URL."),(0,i.kt)("h4",{id:"introduction-to-pseudo-urls"},"Introduction to pseudo-URLs"),(0,i.kt)("p",null,"Pseudo-URLs are represented by our ",(0,i.kt)("inlineCode",{parentName:"p"},"PseudoUrl")," class and even though the name sounds discouraging, they're a pretty simple concept. They're just URLs\nwith some parts replaced by wildcards (read ",(0,i.kt)("a",{href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions"},"regular\nexpressions"),"). They are matched against URLs to find specific links, domains, patterns, file extensions and so on."),(0,i.kt)("p",null,"In scraping, there are usually patterns to be found in website URLs that can be leveraged to scrape only the pages we're interested in. Imagine\na typical online store. It has different categories which list different items The URL for might looks something like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://www.online-store.com/categories\n")),(0,i.kt)("p",null,"A category would then have a different URL:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://www.online-store.com/categories/computers\n")),(0,i.kt)("p",null,"Going to this page would produce a list of offered computers. Then, clicking on one of the computers might take us to a detail URL:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://www.online-store.com/items/613804\n")),(0,i.kt)("p",null,"As you can see, there's a structure to the links. In the real world, the structure might not always be perfectly obvious, but it's very often there.\nPseudo-URLs help to use this structure to select only the relevant links from a given page."),(0,i.kt)("h4",{id:"structure-of-a-pseudo-url"},"Structure of a pseudo-URL"),(0,i.kt)("p",null,"A pseudo-URL is a URL with ",(0,i.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions",target:"_blank",rel:"noopener"},"regular expressions"),") enclosed\nin ",(0,i.kt)("inlineCode",{parentName:"p"},"[]")," brackets. Since we're running Node.js, the regular expressions should follow the JavaScript style."),(0,i.kt)("p",null,"For example, a pseudo-URL"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://www.online-store.com/categories/[(\\w|-)+]\n")),(0,i.kt)("p",null,"will match all of the following URLs:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://www.online-store.com/categories/computers\nhttps://www.online-store.com/categories/mobile-phones\nhttps://www.online-store.com/categories/black-friday\n")),(0,i.kt)("p",null,"but it will not match"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://www.online-store.com/categories\nhttps://www.online-store.com/items/613804\n")),(0,i.kt)("p",null,"This way, you can easily find just the URLs that you're looking for while ignoring the rest."),(0,i.kt)("p",null,"A pseudo-URL may include any number of bracketed regular expressions, so you can compose much more complex matching logic. The following Pseudo URL\nwill match the items in the store even if the links use the non-secure ",(0,i.kt)("inlineCode",{parentName:"p"},"http")," protocol, omit the ",(0,i.kt)("inlineCode",{parentName:"p"},"www")," from the hostname or use a different TLD."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"http[s?]://[(www)?\\.]online-store.[com|net|org]/items/[\\d+]\n")),(0,i.kt)("p",null,"will match any combination of:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"http://www.online-store.org/items/12345\nhttps://online-store.com/items/633423\nhttp://online-store.net/items/7003\n")),(0,i.kt)("p",null,"but it will not match:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"http://shop.online-store.org/items/12345\nhttps://www.online-store.com/items/calculator\nwww.online-store.org/items/7003\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Pssst! Don't tell anyone, but you can create ",(0,i.kt)("inlineCode",{parentName:"p"},"PseudoUrls")," with plain old ",(0,i.kt)("inlineCode",{parentName:"p"},"RegExp")," instances instead of this brackety madness as well.")),(0,i.kt)("h4",{id:"using-enqueuelinks-to-filter-links"},"Using ",(0,i.kt)("inlineCode",{parentName:"h4"},"enqueueLinks()")," to filter links"),(0,i.kt)("p",null,"That's been quite a lot of theory and examples. We might as well put it to practice. Going back to our ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler")," exercise, we still have number\n2 left to cross off the list - filter links pointing to ",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com"),". We've already shown that at the very least, the ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," function needs\ntwo arguments. The source, in our case the ",(0,i.kt)("inlineCode",{parentName:"p"},"$")," object, and the destination - the ",(0,i.kt)("inlineCode",{parentName:"p"},"requestQueue"),". To filter links, we need to add a third argument:\n",(0,i.kt)("inlineCode",{parentName:"p"},"pseudoUrls"),"."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"options.pseudoUrls")," argument is always an ",(0,i.kt)("inlineCode",{parentName:"p"},"Array"),", but its contents can take on many forms. ",(0,i.kt)("a",{parentName:"p",href:"../api/utils#enqueueLinks",target:null,rel:null},"See the reference"),"\nfor all of them. Since we just need to filter out same domain links, we'll keep it simple and use a pseudo-URL ",(0,i.kt)("inlineCode",{parentName:"p"},"string"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// Assuming previous existence of the '$' and 'requestQueue' variables.\nconst options = {\n    $,\n    requestQueue,\n    pseudoUrls: ['http[s?]://apify.com[.*]'],\n};\n\nawait enqueueLinks(options);\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"To break the pseudo-URL string down, we're looking for both ",(0,i.kt)("inlineCode",{parentName:"p"},"http")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"https")," protocols and the links may only lead to ",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com")," domain. The\nfinal brackets ",(0,i.kt)("inlineCode",{parentName:"p"},"[.*]")," allow everything, so ",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com/contact")," as well as ",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com/store")," will match. If this sounds complex to you, we suggest\n",(0,i.kt)("a",{parentName:"p",href:"https://www.regular-expressions.info/tutorial.html",target:"_blank",rel:"noopener"},"reading a tutorial")," or two on regular expression syntax.")),(0,i.kt)("h4",{id:"resolving-relative-urls-with-enqueuelinks"},"Resolving relative URLs with ",(0,i.kt)("inlineCode",{parentName:"h4"},"enqueueLinks()")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"TLDR;")," Just use ",(0,i.kt)("inlineCode",{parentName:"p"},"baseUrl: request.loadedUrl")," when working with ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),"."),(0,i.kt)("p",null,"This is probably the weirdest and most complicated addition to the list. This is not the place to talk at length about\n",(0,i.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/2005079/absolute-vs-relative-urls",target:"_blank",rel:"noopener"},"absolute and relative paths"),", but in short, the\nlinks we encounter in a page can either be absolute, such as:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://apify.com/john-doe/my-actor\n")),(0,i.kt)("p",null,"or relative:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"./john-doe/my-actor\n")),(0,i.kt)("p",null,"Browsers handle this automatically, but since we're only using plain HTTP requests, we need to tell the ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," function how to resolve the\nrelative links to the absolute ones, so we can use them for scraping. This is where the ",(0,i.kt)("inlineCode",{parentName:"p"},"request.loadedUrl")," comes into play, because it returns the\ncorrect URL to use as ",(0,i.kt)("inlineCode",{parentName:"p"},"baseUrl"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// Assuming previous existence of the '$', 'requestQueue' and 'request' variables.\nconst options = {\n    $,\n    requestQueue,\n    pseudoUrls: ['http[s?]://apify.com[.*]'],\n    baseUrl: request.loadedUrl,\n};\n\nawait enqueueLinks(options);\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Even though it seems possible, we can't use the ",(0,i.kt)("inlineCode",{parentName:"p"},"request.url")," of our ",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," instances, because the page could have been redirected and the final\nURL would be different from the one we requested.")),(0,i.kt)("h4",{id:"integrating-enqueuelinks-into-our-crawler"},"Integrating ",(0,i.kt)("inlineCode",{parentName:"h4"},"enqueueLinks()")," into our crawler"),(0,i.kt)("p",null,"That was fairly easy, wasn't it. That ticks number 2 off our list and we're done! Let's take a look at the original crawler code, where we\nenqueued all the links manually."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const { URL } = require('url'); // <------ This is new.\nconst Apify = require('apify');\n\nApify.main(async () => {\n    const requestQueue = await Apify.openRequestQueue();\n    await requestQueue.addRequest({ url: 'https://apify.com' });\n\n    const handlePageFunction = async ({ request, $ }) => {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // Here starts the new part of handlePageFunction.\n        const links = $('a[href]')\n            .map((i, el) => $(el).attr('href'))\n            .get();\n\n        const ourDomain = 'https://apify.com';\n        const absoluteUrls = links.map(link => new URL(link, ourDomain));\n\n        const sameDomainLinks = absoluteUrls.filter(url => url.href.startsWith(ourDomain));\n\n        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);\n        for (const url of sameDomainLinks) {\n            await requestQueue.addRequest({ url: url.href });\n        }\n    };\n\n    const crawler = new Apify.CheerioCrawler({\n        maxRequestsPerCrawl: 20, // <------ This is new too.\n        requestQueue,\n        handlePageFunction,\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("p",null,"Since we've already prepared the ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," options, we can just replace all the above enqueuing logic with a single function call, as promised."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\nconst {\n    utils: { enqueueLinks },\n} = Apify;\n\nApify.main(async () => {\n    const requestQueue = await Apify.openRequestQueue();\n    await requestQueue.addRequest({ url: 'https://apify.com' });\n\n    const handlePageFunction = async ({ request, $ }) => {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // Enqueue links\n        const enqueued = await enqueueLinks({\n            $,\n            requestQueue,\n            pseudoUrls: ['http[s?]://apify.com[.*]'],\n            baseUrl: request.loadedUrl,\n        });\n        console.log(`Enqueued ${enqueued.length} URLs.`);\n    };\n\n    const crawler = new Apify.CheerioCrawler({\n        maxRequestsPerCrawl: 20,\n        requestQueue,\n        handlePageFunction,\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("p",null,"And that's it! No more parsing the links from HTML using Cheerio, filtering them and enqueueing them one by one It all gets done automatically!\n",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," is just one example of Apify SDK's powerful helper functions. They're all designed to make your life easier so you can focus on\ngetting your data, while leaving the mundane crawling management to your tools."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"Apify.utils.enqueueLinks()")," has a lot more tricks up its sleeve. Make sure to check out the\n",(0,i.kt)("a",{parentName:"p",href:"../api/utils#enqueueLinks",target:null,rel:null},"reference documentation")," to see what else it can do for you. Namely the feature to prepopulate the ",(0,i.kt)("inlineCode",{parentName:"p"},"Request"),"\ninstances it creates with ",(0,i.kt)("inlineCode",{parentName:"p"},"userData")," of your choice is extremely useful!"),(0,i.kt)("h2",{id:"getting-some-real-world-data"},"Getting some real-world data"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Hey, guys, you know, it's cool that we can scrape the ",(0,i.kt)("inlineCode",{parentName:"p"},"<title>")," elements of web pages, but that's not very useful. Can we finally scrape some real\ndata and save it somewhere in a machine readable format? Because that's why you started reading this tutorial in the first place!")),(0,i.kt)("p",null,"We hear you, young padawan! First, learn how to crawl, you must. Only then, walk through data, you can!"),(0,i.kt)("p",null,"###\xa0Making a store crawler"),(0,i.kt)("p",null,"Fortunately, we don't have to travel to a galaxy far far away to find a good candidate for learning how to scrape\nstructured data. The ",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/store",target:"_blank",rel:"noopener"},"Apify Store")," is a library of public actors that anyone can grab and use. You\ncan find ready-made solutions for crawling ",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/drobnikj/crawler-google-places",target:"_blank",rel:"noopener"},"Google Places"),",\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/vaclavrut/amazon-crawler",target:"_blank",rel:"noopener"},"Amazon"),",\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/apify/google-search-scraper",target:"_blank",rel:"noopener"},"Google Search"),",\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/dtrungtin/booking-scraper",target:"_blank",rel:"noopener"},"Booking"),",\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/jaroslavhejlek/instagram-scraper",target:"_blank",rel:"noopener"},"Instagram"),",\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/maxcopell/tripadvisor",target:"_blank",rel:"noopener"},"Tripadvisor")," and many other websites. Feel free to check them out! It's also a great place to practice our Jedi scraping skills since it has categories, lists and details. That's almost like our imaginary\n",(0,i.kt)("inlineCode",{parentName:"p"},"online-store.com")," from the previous chapter."),(0,i.kt)("h3",{id:"the-importance-of-having-a-plan"},"The importance of having a plan"),(0,i.kt)("p",null,'Sometimes scraping is really straightforward, but most of the times, it really pays to do a little bit of research first. How is the website\nstructured? Can I scrape it only with HTTP requests (read "with ',(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),"\") or would I need a full browser solution? Are there any\nanti-scraping protections in place? Do I need to parse the HTML or can I get the data otherwise, such as directly from the website's API. Jakub,\none of Apify's founders, wrote a\n",(0,i.kt)("a",{parentName:"p",href:"https://blog.apify.com/web-scraping-in-2018-forget-html-use-xhrs-metadata-or-javascript-variables-8167f252439c",target:"_blank",rel:"noopener"},"great article about all the different techniques"),"\nand tips and tricks, so make sure to check that out!"),(0,i.kt)("p",null,"For the purposes of this tutorial, let's just go ahead with HTTP requests and HTML parsing using ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),". The number one reason being: We\nalready know how to use it and we want to build on that knowledge to learn specific crawling and scraping techniques."),(0,i.kt)("h4",{id:"choosing-the-data-we-need"},"Choosing the data we need"),(0,i.kt)("p",null,"A good first step is always to figure out what it is we want to scrape and where to find it. For the time being, let's just agree that we want to\nscrape all actors (see the ",(0,i.kt)("inlineCode",{parentName:"p"},"Show")," dropdown) in all categories (which can be found on the left side of the page) and for each actor we want to get its"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"URL"),(0,i.kt)("li",{parentName:"ol"},"Owner"),(0,i.kt)("li",{parentName:"ol"},"Unique identifier (such as ",(0,i.kt)("inlineCode",{parentName:"li"},"apify/web-scraper"),")"),(0,i.kt)("li",{parentName:"ol"},"Title"),(0,i.kt)("li",{parentName:"ol"},"Description"),(0,i.kt)("li",{parentName:"ol"},"Last modification date"),(0,i.kt)("li",{parentName:"ol"},"Number of runs")),(0,i.kt)("p",null,'We can see that some of the information is available directly on the list page, but for details such as "Last modification date" or "Number of runs" we\'ll also need\nto open the actor detail pages.'),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"data to scrape",src:n(35631).Z,title:"Overview of data to be scraped.",width:"1600",height:"1215"})),(0,i.kt)("h4",{id:"analyzing-the-target"},"Analyzing the target"),(0,i.kt)("p",null,"Knowing that we will use plain HTTP requests, we immediately know that we won't be able to manipulate the website in any way. We will only be able to\ngo through the HTML it gives us and parse our data from there. This might sound like a huge limitation, but you might be surprised in how effective it\ncan be. Let's get to it!"),(0,i.kt)("h4",{id:"the-start-urls"},"The start URL(s)"),(0,i.kt)("p",null,"This is where we start our crawl. It's convenient to start as close to our data as possible. For example, it wouldn't make much sense to start at\n",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com")," and look for a ",(0,i.kt)("inlineCode",{parentName:"p"},"store")," link there, when we already know that everything we want to extract can be found at the ",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com/store")," page."),(0,i.kt)("p",null,"Once we look at the ",(0,i.kt)("inlineCode",{parentName:"p"},"apify.com/store")," page more carefully, we see that the categories themselves produce URLs that we can use to access those\nindividual categories."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://apify.com/store?category=ENTERTAINMENT\n")),(0,i.kt)("p",null,"Should we write down all the category URLs down and use all of them as start URLs? It's definitely possible, but what if a new category appears on the\npage later? We would not learn about it unless we manually visit the page and inspect it again. So scraping the category links off the store page\ndefinitely makes sense. This way we always get an up to date list of categories."),(0,i.kt)("p",null,"But is it really that straightforward? By digging further into the store page's HTML we find that it does not actually contain the category links. The\nmenu on the left uses JavaScript to display the items from a given category and, as we've learned earlier, ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler")," cannot execute JavaScript."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We've deliberately chosen this scenario to show an example of the number one weakness of ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),". We will overcome this difficulty in our\n",(0,i.kt)("inlineCode",{parentName:"p"},"PuppeteerCrawler")," tutorial, but at the cost of compute resources and speed. Always remember that no tool is best for everything!")),(0,i.kt)("p",null,"So we're back to the pre-selected list of URLs. Since we cannot scrape the list dynamically, we have to manually collect the links and then use them\nin our crawler. We lose the ability to scrape new categories, but we keep the low resource consumption and speed advantages of ",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),"."),(0,i.kt)("p",null,"Therefore, after careful consideration, we've determined that we should use multiple start URLs and that they should look as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://apify.com/store?category=TRAVEL\nhttps://apify.com/store?category=ECOMMERCE\nhttps://apify.com/store?category=ENTERTAINMENT\n")),(0,i.kt)("h3",{id:"the-crawling-strategy"},"The crawling strategy"),(0,i.kt)("p",null,"Now that we know where to start, we need to figure out where to go next. Since we've eliminated one level of crawling by selecting the categories\nmanually, we now only need to crawl the actor detail pages. The algorithm therefore follows:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Visit the category list page (one of our start URLs)."),(0,i.kt)("li",{parentName:"ol"},"Enqueue all links to actor details."),(0,i.kt)("li",{parentName:"ol"},"Visit all actor details and extract data."),(0,i.kt)("li",{parentName:"ol"},"Repeat 1 - 3 for all categories.")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Technically, this is a depth first crawl and the crawler will perform a breadth first crawl by default, but that's an implementation detail. We've\nchosen this notation since a breadth first crawl would be less readable.")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler")," will make sure to visit the pages for us, if we provide the correct ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," and we already know how to enqueue pages, so this\nshould be fairly easy. Nevertheless, there are two more tricks that we'd like to show you."),(0,i.kt)("h4",{id:"using-a-requestlist"},"Using a ",(0,i.kt)("inlineCode",{parentName:"h4"},"RequestList")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," is a perfect tool for scraping a pre-existing list of URLs and if you think about our start URLs, this is exactly what we have! A list\nof links to the different categories of the store. Let's see how we'd get them into a ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const sources = [\n    'https://apify.com/store?category=TRAVEL',\n    'https://apify.com/store?category=ECOMMERCE',\n    'https://apify.com/store?category=ENTERTAINMENT',\n];\n\nconst requestList = await Apify.openRequestList('categories', sources);\n")),(0,i.kt)("p",null,"As you can see, similarly to the ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.openRequestQueue()")," function, there is an ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.openRequestList()")," function that will create a ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList"),"\ninstance for you. The first argument is the name of the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList"),". It is used to persist the crawling state of the list. This is useful when you\nwant to continue where you left off after an error or a process restart. The second argument is the ",(0,i.kt)("inlineCode",{parentName:"p"},"sources")," array, which is nothing more than a list\nof URLs you wish to crawl."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," is a persistent store by default, so no name is needed, while the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," only lives in memory and giving it a name enables it\nto become persistent.")),(0,i.kt)("p",null,"You might now want to ask one of these questions:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Can I enqueue into ",(0,i.kt)("inlineCode",{parentName:"li"},"RequestList")," too?"),(0,i.kt)("li",{parentName:"ul"},"How do I make ",(0,i.kt)("inlineCode",{parentName:"li"},"RequestList")," work together with ",(0,i.kt)("inlineCode",{parentName:"li"},"RequestQueue")," since I need the queue to enqueue new ",(0,i.kt)("inlineCode",{parentName:"li"},"Requests"),".")),(0,i.kt)("p",null,"The answer to the first one is a definitive no. ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," is immutable and once you create it, you cannot add or remove ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," from it. The\nanswer to the second one is simple. ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," are made to work together out of the box in crawlers, so all you need to do is\nuse them both and the crawlers will do the rest."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const crawler = new Apify.CheerioCrawler({\n    requestList,\n    requestQueue,\n    handlePageFunction,\n});\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"For those wondering how this works, the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," are enqueued into the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," right before their execution and only\nprocessed by the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," afterwards. You can, of course, enqueue the ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," to the queue manually, but that would take some boilerplate\ncode and perhaps quite a long time, if we were talking about tens of thousands or more ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests"),". The crawlers do it while running, so the time to\nenqueue is spread out and you won't even notice it.")),(0,i.kt)("p",null,"####\xa0Sanity check"),(0,i.kt)("p",null,"It's always useful to create some simple boilerplate code to see that we've got everything set up correctly before we start to write\nthe scraping logic itself. We might realize that something in our previous analysis doesn't quite add up, or the website might not behave exactly as we expected."),(0,i.kt)("p",null,"Let's use our newly acquired ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList")," knowledge and everything we know from the previous chapters to create a new crawler that'll just visit all\nthe category URLs we selected and print the text content of all the actors in the category. Try running the code below in your selected environment.\nYou should see, albeit very badly formatted, the text of the individual actor cards that are displayed in the selected categories."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n\nApify.main(async () => {\n    const sources = [\n        'https://apify.com/store?category=TRAVEL',\n        'https://apify.com/store?category=ECOMMERCE',\n        'https://apify.com/store?category=ENTERTAINMENT',\n    ];\n\n    const requestList = await Apify.openRequestList('categories', sources);\n\n    const crawler = new Apify.CheerioCrawler({\n        requestList,\n        handlePageFunction: async ({ $, request }) => {\n            // Select all the actor cards.\n            $('.item').each((i, el) => {\n                const text = $(el).text();\n                console.log(`ITEM: ${text}\\n`);\n            });\n        },\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"If there's anything you don't understand, refer to the previous chapters on setting up your environment, building your first crawler and\n",(0,i.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),".")),(0,i.kt)("p",null,"You might be wondering how we got that ",(0,i.kt)("inlineCode",{parentName:"p"},".item")," selector. After analyzing the category pages using a browser's DevTools, we've determined that it's a\ngood selector to select all the currently displayed actor cards. DevTools and CSS selectors are quite a large topic, so we can't go into too much\ndetail now, but here are a few general pointers."),(0,i.kt)("h4",{id:"devtools-crash-course"},"DevTools crash course"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We'll use Chrome DevTools here, since it's the most common browser, but feel free to use any other, it's all very similar.")),(0,i.kt)("p",null,"We could pick any category, but let's just go with Travel because it includes some interesting actors. Go to"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"https://apify.com/store?category=TRAVEL\n")),(0,i.kt)("p",null,"and open DevTools either by right-clicking anywhere in the page and selecting ",(0,i.kt)("inlineCode",{parentName:"p"},"Inspect"),", or by pressing ",(0,i.kt)("inlineCode",{parentName:"p"},"F12")," or by any other means relevant to your\nsystem. Once you're there, you'll see a bunch of DevToolsy stuff and a view of the category page with the individual actor cards."),(0,i.kt)("p",null,"Now, find the ",(0,i.kt)("inlineCode",{parentName:"p"},"Select an element")," tool and use it to select one of the actor cards. Make sure to select the whole card, not just some of its contents, such\nas its title or description."),(0,i.kt)("p",null,"In the resulting HTML display, it will put your cursor somewhere. Inspect the HTML around it. You'll see that there are CSS classes attached to the\ndifferent HTML elements."),(0,i.kt)("p",null,"By hovering over the individual elements, you will see their placement in the page's view. It's easy to see the page's structure around the actor\ncards now. All the cards are displayed in a ",(0,i.kt)("inlineCode",{parentName:"p"},"<div>")," with a classname that starts with ",(0,i.kt)("inlineCode",{parentName:"p"},"ItemsGrid__StyledDiv"),", which holds another ",(0,i.kt)("inlineCode",{parentName:"p"},"<div>")," with some\ncomputer-generated class names and finally, inside this ",(0,i.kt)("inlineCode",{parentName:"p"},"<div>"),", the individual cards are represented by other ",(0,i.kt)("inlineCode",{parentName:"p"},"<div>")," elements with the class of\n",(0,i.kt)("inlineCode",{parentName:"p"},"item"),"."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Yes, there are other HTML elements and other classes too. We can safely ignore them.")),(0,i.kt)("p",null,"It should now make sense how we got that ",(0,i.kt)("inlineCode",{parentName:"p"},".item")," selector. It's just a selector that finds all elements that are annotated with the ",(0,i.kt)("inlineCode",{parentName:"p"},"item")," class and\nthose just happen to be the actor cards only."),(0,i.kt)("p",null,"It's always a good idea to double check that though, so go into the DevTools Console and run"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"document.querySelectorAll('.item');\n")),(0,i.kt)("p",null,"You will see that only the actor cards will be returned, and nothing else."),(0,i.kt)("h4",{id:"enqueueing-the-detail-links-using-a-custom-selector"},"Enqueueing the detail links using a custom selector"),(0,i.kt)("p",null,"In the previous chapter, we used the ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.utils.enqueueLinks()")," function like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"await enqueueLinks({\n    $,\n    requestQueue,\n    pseudoUrls: ['http[s?]://apify.com[.*]'],\n    baseUrl: request.loadedUrl,\n});\n")),(0,i.kt)("p",null,"While very useful in that scenario, we need something different now. Instead of finding all the ",(0,i.kt)("inlineCode",{parentName:"p"},'<a href="..">')," links that match the ",(0,i.kt)("inlineCode",{parentName:"p"},"pseudoUrl"),", we\nneed to find only the specific ones that will take us to the actor detail pages. Otherwise, we'd be visiting a lot of other pages that we're not\ninterested in. Using the power of DevTools and yet another ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," parameter, this becomes fairly easy."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const handlePageFunction = async ({ $, request }) => {\n    console.log(`Processing ${request.url}`);\n\n    // Only enqueue new links from the category pages.\n    if (!request.userData.detailPage) {\n        await Apify.utils.enqueueLinks({\n            $,\n            requestQueue,\n            selector: 'div.item > a',\n            baseUrl: request.loadedUrl,\n            transformRequestFunction: req => {\n                req.userData.detailPage = true;\n                return req;\n            },\n        });\n    }\n};\n")),(0,i.kt)("p",null,"The code should look pretty familiar to you. It's a very simple ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," where we log the currently processed URL to the console and\nenqueue more links. But there are also a few new, interesting additions. Let's break it down."),(0,i.kt)("h5",{id:"the-selector-parameter-of-enqueuelinks"},"The ",(0,i.kt)("inlineCode",{parentName:"h5"},"selector")," parameter of ",(0,i.kt)("inlineCode",{parentName:"h5"},"enqueueLinks()")),(0,i.kt)("p",null,"When we previously used ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()"),", we were not providing any ",(0,i.kt)("inlineCode",{parentName:"p"},"selector")," parameter and it was fine, because we wanted to use the default\nsetting, which is ",(0,i.kt)("inlineCode",{parentName:"p"},"a")," - finds all ",(0,i.kt)("inlineCode",{parentName:"p"},"<a>")," elements. But now, we need to be more specific. There are multiple ",(0,i.kt)("inlineCode",{parentName:"p"},"<a>")," links on the given category page, but\nwe're only interested in those that will take us to item (actor) details. Using the DevTools, we found out that we can select the links we wanted\nusing the ",(0,i.kt)("inlineCode",{parentName:"p"},"div.item > a")," selector, which selects all the ",(0,i.kt)("inlineCode",{parentName:"p"},"<a>")," elements that have a ",(0,i.kt)("inlineCode",{parentName:"p"},'<div class="item ...">')," parent. And those are exactly the ones\nwe're interested in."),(0,i.kt)("h5",{id:"the-missing-pseudourls"},"The missing ",(0,i.kt)("inlineCode",{parentName:"h5"},"pseudoUrls")),(0,i.kt)("p",null,"Earlier we learned that ",(0,i.kt)("inlineCode",{parentName:"p"},"pseudoUrls")," are not required and if omitted, all links matching the given ",(0,i.kt)("inlineCode",{parentName:"p"},"selector")," will be enqueued. This is exactly\nwhat we need, so we're skipping ",(0,i.kt)("inlineCode",{parentName:"p"},"pseudoUrls")," this time. That does not mean that you can't use ",(0,i.kt)("inlineCode",{parentName:"p"},"pseudoUrls")," together with a custom ",(0,i.kt)("inlineCode",{parentName:"p"},"selector")," though,\nbecause you absolutely can!"),(0,i.kt)("h5",{id:"finally-the-userdata-of-enqueuelinks"},"Finally, the ",(0,i.kt)("inlineCode",{parentName:"h5"},"userData")," of ",(0,i.kt)("inlineCode",{parentName:"h5"},"enqueueLinks()")),(0,i.kt)("p",null,"You will see ",(0,i.kt)("inlineCode",{parentName:"p"},"userData")," used often throughout Apify SDK and it's nothing more than a place to store your own data on a ",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," instance. You can\naccess it with ",(0,i.kt)("inlineCode",{parentName:"p"},"request.userData")," and it's a plain ",(0,i.kt)("inlineCode",{parentName:"p"},"Object")," that can be used to store anything that needs to survive the full life-cycle of the\n",(0,i.kt)("inlineCode",{parentName:"p"},"Request"),"."),(0,i.kt)("p",null,"We can use the ",(0,i.kt)("inlineCode",{parentName:"p"},"transformRequestFunction")," option of ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," to modify all the ",(0,i.kt)("inlineCode",{parentName:"p"},"Request")," instances it creates and enqueues. In our case, we\nuse it to set a ",(0,i.kt)("inlineCode",{parentName:"p"},"detailPage")," property to the enqueued ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," to be able to easily differentiate between the category pages and the detail pages."),(0,i.kt)("h4",{id:"another-sanity-check"},"Another sanity check"),(0,i.kt)("p",null,"It's always good to work step by step. We have this new enqueueing logic in place and since the previous ",(0,i.kt)("a",{parentName:"p",href:"#sanity-check",target:null,rel:null},"Sanity check")," worked only\nwith a ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestList"),", because we were not enqueueing anything, don't forget to add back the ",(0,i.kt)("inlineCode",{parentName:"p"},"RequestQueue")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"maxRequestsPerCrawl")," limit. Let's\ntest it out!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n\nApify.main(async () => {\n    const sources = [\n        'https://apify.com/store?category=TRAVEL',\n        'https://apify.com/store?category=ECOMMERCE',\n        'https://apify.com/store?category=ENTERTAINMENT',\n    ];\n\n    const requestList = await Apify.openRequestList('categories', sources);\n    const requestQueue = await Apify.openRequestQueue(); // <----------------\n\n    const crawler = new Apify.CheerioCrawler({\n        maxRequestsPerCrawl: 50, // <----------------------------------------\n        requestList,\n        requestQueue, // <---------------------------------------------------\n        handlePageFunction: async ({ $, request }) => {\n            console.log(`Processing ${request.url}`);\n\n            // Only enqueue new links from the category pages.\n            if (!request.userData.detailPage) {\n                await Apify.utils.enqueueLinks({\n                    $,\n                    requestQueue,\n                    selector: 'div.item > a',\n                    baseUrl: request.loadedUrl,\n                    transformRequestFunction: req => {\n                        req.userData.detailPage = true;\n                        return req;\n                    },\n                });\n            }\n        },\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("p",null,"We've added the ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction()")," with the ",(0,i.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," logic from the previous section to the code we wrote earlier. As always, try\nrunning it in the environment of your choice. You should see the crawler output a number of links to the console, as it crawls the category pages first\nand then all the links to the actor detail pages it found."),(0,i.kt)("p",null,"This concludes our Crawling strategy section, because we have taught the crawler to visit all the pages we need. Let's continue with scraping the\ntasty data."),(0,i.kt)("h3",{id:"scraping-data"},"Scraping data"),(0,i.kt)("p",null,"At the beginning of this chapter, we created a list of the information we wanted to collect about the actors in the store. Let's review that and figure\nout ways to access it."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"URL"),(0,i.kt)("li",{parentName:"ol"},"Owner"),(0,i.kt)("li",{parentName:"ol"},"Unique identifier (such as ",(0,i.kt)("inlineCode",{parentName:"li"},"apify/web-scraper"),")"),(0,i.kt)("li",{parentName:"ol"},"Title"),(0,i.kt)("li",{parentName:"ol"},"Description"),(0,i.kt)("li",{parentName:"ol"},"Last modification date"),(0,i.kt)("li",{parentName:"ol"},"Number of runs")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"data to scrape",src:n(35631).Z,title:"Overview of data to be scraped.",width:"1600",height:"1215"})),(0,i.kt)("h4",{id:"scraping-the-url-owner-and-unique-identifier"},"Scraping the URL, Owner and Unique identifier"),(0,i.kt)("p",null,"Some information is lying right there in front of us without even having to touch the actor detail pages. The ",(0,i.kt)("inlineCode",{parentName:"p"},"URL")," we already have - the\n",(0,i.kt)("inlineCode",{parentName:"p"},"request.url"),". And by looking at it carefully, we realize that it already includes the ",(0,i.kt)("inlineCode",{parentName:"p"},"owner")," and the ",(0,i.kt)("inlineCode",{parentName:"p"},"unique identifier")," too. We can just split the\n",(0,i.kt)("inlineCode",{parentName:"p"},"string")," and be on our way then!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// request.url = https://apify.com/apify/web-scraper\n\nconst urlArr = request.url.split('/').slice(-2); // ['apify', 'web-scraper']\nconst uniqueIdentifier = urlArr.join('/'); // 'apify/web-scraper'\nconst owner = urlArr[0]; // 'apify'\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"It's always a matter of preference, whether to store this information separately in the resulting dataset, or not. Whoever uses the dataset can\neasily parse the ",(0,i.kt)("inlineCode",{parentName:"p"},"owner")," from the ",(0,i.kt)("inlineCode",{parentName:"p"},"URL"),", so should we duplicate the data unnecessarily? Our opinion is that unless the increased data consumption\nwould be too large to bear, it's always better to make the dataset as readable as possible. Someone might want to filter by ",(0,i.kt)("inlineCode",{parentName:"p"},"owner")," for example and\nkeeping only the ",(0,i.kt)("inlineCode",{parentName:"p"},"URL")," in the dataset would make this complicated without using additional tools.")),(0,i.kt)("h4",{id:"scraping-title-description-last-modification-date-and-number-of-runs"},"Scraping Title, Description, Last modification date and Number of runs"),(0,i.kt)("p",null,"Now it's time to add more data to the results. Let's open one of the actor detail pages in the Store, for example the\n",(0,i.kt)("a",{parentName:"p",href:"https://apify.com/apify/web-scraper",target:"_blank",rel:"noopener"},(0,i.kt)("inlineCode",{parentName:"a"},"apify/web-scraper"))," page and use our DevTools-Fu to figure out how to get the title of the actor."),(0,i.kt)("h5",{id:"title"},"Title"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"actor title",src:n(67543).Z,title:"Finding actor title in DevTools.",width:"1600",height:"1215"})),(0,i.kt)("p",null,"By using the element selector tool, we find out that the title is there under an ",(0,i.kt)("inlineCode",{parentName:"p"},"<h1>")," tag, as titles should be.\nMaybe surprisingly, we find that there are actually two ",(0,i.kt)("inlineCode",{parentName:"p"},"<h1>")," tags on the detail page. This should get us thinking.\nIs there any parent element that includes our ",(0,i.kt)("inlineCode",{parentName:"p"},"<h1>")," tag, but not the other ones? Yes, there is! There is a ",(0,i.kt)("inlineCode",{parentName:"p"},"<header>"),"\nelement that we can use to select only the heading we're interested in."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Remember that you can press CTRL+F (CMD+F) in the Elements tab of DevTools to open the search bar where you can quickly search for elements using\ntheir selectors. And always make sure to use the DevTools to verify your scraping process and assumptions. It's faster than changing the crawler\ncode all the time.")),(0,i.kt)("p",null,"To get the title we just need to find it using ",(0,i.kt)("inlineCode",{parentName:"p"},"Cheerio")," and a ",(0,i.kt)("inlineCode",{parentName:"p"},"header h1")," selector, which selects all ",(0,i.kt)("inlineCode",{parentName:"p"},"<h1>")," elements that have a ",(0,i.kt)("inlineCode",{parentName:"p"},"<header>")," ancestor.\nAnd as we already know, there's only one."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"return {\n    title: $('header h1').text(),\n};\n")),(0,i.kt)("h5",{id:"description"},"Description"),(0,i.kt)("p",null,"Getting the actor's description is a little more involved, but still pretty straightforward. We can't just simply search for a ",(0,i.kt)("inlineCode",{parentName:"p"},"<p>")," tag, because\nthere's a lot of them in the page. We need to narrow our search down a little. Using the DevTools we find that the actor description is nested within\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"<header>")," element too, same as the title. Moreover, the actual description is nested inside a ",(0,i.kt)("inlineCode",{parentName:"p"},"<span>")," tag with a class ",(0,i.kt)("inlineCode",{parentName:"p"},"actor-description"),"."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"actor description selector",src:n(12872).Z,title:"Finding actor description in DevTools.",width:"1600",height:"1215"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"return {\n    title: $('header h1').text(),\n    description: $('header span.actor-description').text(),\n};\n")),(0,i.kt)("h5",{id:"last-modification-date"},"Last modification date"),(0,i.kt)("p",null,"The DevTools tell us that the ",(0,i.kt)("inlineCode",{parentName:"p"},"modifiedDate")," can be found in the ",(0,i.kt)("inlineCode",{parentName:"p"},"<time>")," element inside ",(0,i.kt)("inlineCode",{parentName:"p"},'<ul class="ActorHeader-stats">'),"."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"actor last modification date selector",src:n(56281).Z,title:"Finding actor last modification date in DevTools.",width:"1600",height:"1215"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"return {\n    title: $('header h1').text(),\n    description: $('header span.actor-description').text(),\n    modifiedDate: new Date(\n        Number(\n            $('ul.ActorHeader-stats time').attr('datetime'),\n        ),\n    ),\n};\n")),(0,i.kt)("p",null,"It might look a little too complex at first glance, but let's walk through it. We find the right ",(0,i.kt)("inlineCode",{parentName:"p"},"<time>")," element,\nand then we read its ",(0,i.kt)("inlineCode",{parentName:"p"},"datetime")," attribute, because that's where a unix timestamp is stored as a ",(0,i.kt)("inlineCode",{parentName:"p"},"string"),"."),(0,i.kt)("p",null,"But we would much rather see a readable date in our results, not a unix timestamp, so we need to convert it. Unfortunately the ",(0,i.kt)("inlineCode",{parentName:"p"},"new Date()"),"\nconstructor will not accept a ",(0,i.kt)("inlineCode",{parentName:"p"},"string"),", so we cast the ",(0,i.kt)("inlineCode",{parentName:"p"},"string")," to a ",(0,i.kt)("inlineCode",{parentName:"p"},"number")," using the ",(0,i.kt)("inlineCode",{parentName:"p"},"Number()")," function before actually calling ",(0,i.kt)("inlineCode",{parentName:"p"},"new Date()"),".\nPhew!"),(0,i.kt)("h5",{id:"run-count"},"Run count"),(0,i.kt)("p",null,"And so we're finishing up with the ",(0,i.kt)("inlineCode",{parentName:"p"},"runCount"),". There's no specific element like ",(0,i.kt)("inlineCode",{parentName:"p"},"<time>"),", so we need to create a complex selector and then do a\ntransformation on the result."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"return {\n    title: $('header h1').text(),\n    description: $('header span.actor-description').text(),\n    modifiedDate: new Date(\n        Number(\n            $('ul.ActorHeader-stats time').attr('datetime'),\n        ),\n    ),\n    runCount: Number(\n        $('ul.ActorHeader-stats > li:nth-of-type(3)')\n            .text()\n            .match(/[\\d,]+/)[0]\n            .replace(',', ''),\n    ),\n};\n")),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"ul.ActorHeader-stats > li:nth-of-type(3)")," looks complicated, but it only reads that we're looking for a ",(0,i.kt)("inlineCode",{parentName:"p"},'<ul class="ActorHeader-stats ...">')," element and within that\nelement we're looking for the third ",(0,i.kt)("inlineCode",{parentName:"p"},"<li>")," element. We grab its text, but we're only interested in the number of runs. So we parse the number out\nusing a regular expression, but its type is still a ",(0,i.kt)("inlineCode",{parentName:"p"},"string"),", so we finally convert the result to a ",(0,i.kt)("inlineCode",{parentName:"p"},"number")," by wrapping it with a ",(0,i.kt)("inlineCode",{parentName:"p"},"Number()")," call."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"The numbers are formatted with commas as thousands separators (e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"'1,234,567'"),"), so to extract it, we\nfirst use regular expression ",(0,i.kt)("inlineCode",{parentName:"p"},"/[\\d,]+/")," - it will search for consecutive number or comma characters.\nThen we extract the match via ",(0,i.kt)("inlineCode",{parentName:"p"},".match(/[\\d,]+/)[0]")," and finally remove the commas by calling ",(0,i.kt)("inlineCode",{parentName:"p"},".replace(',', '')"),".\nThis will give us a string (e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"'1234567'"),") that can be converted via ",(0,i.kt)("inlineCode",{parentName:"p"},"Number")," function.")),(0,i.kt)("p",null,"And there we have it! All the data we needed in a single object. For the sake of completeness, let's add the properties we parsed from the URL earlier\nand we're good to go."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const urlArr = request.url.split('/').slice(-2);\n\nconst results = {\n    url: request.url,\n    uniqueIdentifier: urlArr.join('/'),\n    owner: urlArr[0],\n    title: $('header h1').text(),\n    description: $('header span.actor-description').text(),\n    modifiedDate: new Date(\n        Number(\n            $('ul.ActorHeader-stats time').attr('datetime'),\n        ),\n    ),\n    runCount: Number(\n        $('ul.ActorHeader-stats > li:nth-of-type(3)')\n            .text()\n            .match(/[\\d,]+/)[0]\n            .replace(',', ''),\n    ),\n};\n\nconsole.log('RESULTS: ', results);\n")),(0,i.kt)("h4",{id:"trying-it-out-sanity-check-3"},"Trying it out (sanity check #3)"),(0,i.kt)("p",null,"We have everything we need so just grab our newly created scraping logic, dump it into our original ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction()")," and see the magic happen!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n\nApify.main(async () => {\n    const sources = [\n        'https://apify.com/store?category=TRAVEL',\n        'https://apify.com/store?category=ECOMMERCE',\n        'https://apify.com/store?category=ENTERTAINMENT',\n    ];\n\n    const requestList = await Apify.openRequestList('categories', sources);\n    const requestQueue = await Apify.openRequestQueue();\n\n    const crawler = new Apify.CheerioCrawler({\n        maxRequestsPerCrawl: 50,\n        requestList,\n        requestQueue,\n        handlePageFunction: async ({ $, request }) => {\n            console.log(`Processing ${request.url}`);\n\n            // This is our new scraping logic.\n            if (request.userData.detailPage) {\n                const urlArr = request.url.split('/').slice(-2);\n\n                const results = {\n                    url: request.url,\n                    uniqueIdentifier: urlArr.join('/'),\n                    owner: urlArr[0],\n                    title: $('header h1').text(),\n                    description: $('header span.actor-description').text(),\n                    modifiedDate: new Date(\n                        Number(\n                            $('ul.ActorHeader-stats time').attr('datetime'),\n                        ),\n                    ),\n                    runCount: Number(\n                        $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                            .text()\n                            .match(/[\\d,]+/)[0]\n                            .replace(',', ''),\n                    ),\n                };\n                console.log('RESULTS', results);\n            }\n\n            // Only enqueue new links from the category pages.\n            if (!request.userData.detailPage) {\n                await Apify.utils.enqueueLinks({\n                    $,\n                    requestQueue,\n                    selector: 'div.item > a',\n                    baseUrl: request.loadedUrl,\n                    transformRequestFunction: req => {\n                        req.userData.detailPage = true;\n                        return req;\n                    },\n                });\n            }\n        },\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Notice again that we're scraping on the detail pages ",(0,i.kt)("inlineCode",{parentName:"p"},"request.userData.detailPage === true"),", but we're only enqueueing on the category pages\n",(0,i.kt)("inlineCode",{parentName:"p"},"request.userData.detailPage === undefined"),".")),(0,i.kt)("p",null,"When running the actor in the environment of your choice, you should see the crawled URLs and their scraped data printed to the console."),(0,i.kt)("h3",{id:"saving-the-scraped-data"},"Saving the scraped data"),(0,i.kt)("p",null,"A data extraction job would not be complete without saving the data for later use and processing. We've come to the final and most difficult part of\nthis chapter so make sure to pay attention very carefully!"),(0,i.kt)("p",null,"First, replace the ",(0,i.kt)("inlineCode",{parentName:"p"},"console.log('RESULTS', results)")," call with"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"await Apify.pushData(results);\n")),(0,i.kt)("p",null,"and that's it. Unlike in the previous paragraph, I'm being serious now. That's it, we're done. The final code therefore looks exactly like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const Apify = require('apify');\n\nApify.main(async () => {\n    const sources = [\n        'https://apify.com/store?category=TRAVEL',\n        'https://apify.com/store?category=ECOMMERCE',\n        'https://apify.com/store?category=ENTERTAINMENT',\n    ];\n\n    const requestList = await Apify.openRequestList('categories', sources);\n    const requestQueue = await Apify.openRequestQueue();\n\n    const crawler = new Apify.CheerioCrawler({\n        maxRequestsPerCrawl: 50,\n        requestList,\n        requestQueue,\n        handlePageFunction: async ({ $, request }) => {\n            console.log(`Processing ${request.url}`);\n\n            // This is our new scraping logic.\n            if (request.userData.detailPage) {\n                const urlArr = request.url.split('/').slice(-2);\n\n                const results = {\n                    url: request.url,\n                    uniqueIdentifier: urlArr.join('/'),\n                    owner: urlArr[0],\n                    title: $('header h1').text(),\n                    description: $('header span.actor-description').text(),\n                    modifiedDate: new Date(\n                        Number(\n                            $('ul.ActorHeader-stats time').attr('datetime'),\n                        ),\n                    ),\n                    runCount: Number(\n                        $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                            .text()\n                            .match(/[\\d,]+/)[0]\n                            .replace(',', ''),\n                    ),\n                };\n                await Apify.pushData(results);\n            }\n\n            // Only enqueue new links from the category pages.\n            if (!request.userData.detailPage) {\n                await Apify.utils.enqueueLinks({\n                    $,\n                    requestQueue,\n                    selector: 'div.item > a',\n                    baseUrl: request.loadedUrl,\n                    transformRequestFunction: req => {\n                        req.userData.detailPage = true;\n                        return req;\n                    },\n                });\n            }\n        },\n    });\n\n    await crawler.run();\n});\n")),(0,i.kt)("h4",{id:"whats-apifypushdata"},"What's ",(0,i.kt)("inlineCode",{parentName:"h4"},"Apify.pushData()")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"../api/apify#pushdata",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"Apify.pushData()"))," is a helper function that saves data to the default ",(0,i.kt)("a",{parentName:"p",href:"../api/dataset",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"Dataset")),". ",(0,i.kt)("inlineCode",{parentName:"p"},"Dataset")," is a\nstorage designed to hold virtually unlimited amount of data in a format similar to a table. Each time you call ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.pushData()")," a new row in the\ntable is created, with the property names serving as column titles."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Each actor run has one default ",(0,i.kt)("inlineCode",{parentName:"p"},"Dataset")," so no need to initialize it or create an instance first. It just gets done automatically for you. You can\nalso create named datasets at will.")),(0,i.kt)("h4",{id:"finding-my-saved-data"},"Finding my saved data"),(0,i.kt)("p",null,"It might not be perfectly obvious where the data we saved using the previous command went, so let's break it down by environment:"),(0,i.kt)("h5",{id:"dataset-on-the-apify-platform"},"Dataset on the Apify platform"),(0,i.kt)("p",null,"Open any Run of your actor on the Platform and you will see a Dataset as one of the available tabs. Clicking on it will reveal basic information about\nthe Dataset and a list of options that you can use to download your data. There are various formats such as JSON, XLSX or CSV available and there's\nalso the possibility of downloading only clean items, i.e. a filtered dataset with empty rows and hidden fields removed."),(0,i.kt)("h5",{id:"local-dataset"},"Local Dataset"),(0,i.kt)("p",null,"Unless you changed the environment variables that Apify SDK uses locally, which would suggest that you knew what you were doing and you didn't need\nthis tutorial anyway, you'll find your data in your local Apify Storage."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"{PROJECT_FOLDER}/apify_storage/datasets/default/\n")),(0,i.kt)("p",null,"The above folder will hold all your saved data in numbered files, as they were pushed into the dataset. Each file represents one invocation of\n",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.pushData()")," or one table row."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Unfortunately, the local datasets don't yet support the export in various formats functionality that the Platform Dataset page offers, so for the\ntime being, we're stuck with JSON.")),(0,i.kt)("h3",{id:"final-touch"},"Final touch"),(0,i.kt)("p",null,"It may seem that the data are extracted and the actor is done, but honestly, this is just the beginning. For the sake of brevity, we've completely\nomitted error handling, proxies, debug logging, tests, documentation and other stuff that a reliable software should have. The good thing is, ",(0,i.kt)("strong",{parentName:"p"},"error\nhandling is mostly done by Apify SDK itself"),", so no worries on that front, unless you need some custom magic."),(0,i.kt)("p",null,"Anyway, to spark some ideas, let's look at two more things. First, passing an input to the actor, which will enable us to change the categories we\nwant to scrape without changing the source code itself! And then some refactoring, to show you how we reckon is preferable to structure and annotate\nactor code."),(0,i.kt)("h4",{id:"meet-the-input"},"Meet the ",(0,i.kt)("inlineCode",{parentName:"h4"},"INPUT")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"INPUT")," is just a convention on how we call the actor's input. Because there's no magic in actors, just features, the ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT")," is actually nothing more\nthan a key in the default ",(0,i.kt)("a",{parentName:"p",href:"../api/key-value-store",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"KeyValueStore"))," that's, by convention, used as input on the Apify platform. Also by convention, the\n",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT")," is mostly expected to be of ",(0,i.kt)("inlineCode",{parentName:"p"},"Content-Type: application/json"),"."),(0,i.kt)("p",null,"We will not go into ",(0,i.kt)("inlineCode",{parentName:"p"},"KeyValueStore")," details here, but for the sake of ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT")," you need to remember that there is a function that helps you get it."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"const input = await Apify.getInput();\n")),(0,i.kt)("p",null,"On the Apify Platform, the actor's input that you can set in the Console is automatically saved to the default ",(0,i.kt)("inlineCode",{parentName:"p"},"KeyValueStore")," under the key ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT"),"\nand by calling ",(0,i.kt)("a",{parentName:"p",href:"../api/apify#getvalue",target:null,rel:null},(0,i.kt)("inlineCode",{parentName:"a"},"Apify.getInput()"))," you retrieve the value from the ",(0,i.kt)("inlineCode",{parentName:"p"},"KeyValueStore"),"."),(0,i.kt)("p",null,"Running locally, you need to place an ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT.json")," file in your default key value store for this to work."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"{PROJECT_FOLDER}/apify_storage/key_value_stores/default/INPUT.json\n")),(0,i.kt)("h4",{id:"use-input-to-seed-our-actor-with-categories"},"Use ",(0,i.kt)("inlineCode",{parentName:"h4"},"INPUT")," to seed our actor with categories"),(0,i.kt)("p",null,"Currently we're using the full URLs of categories as sources, but it's quite obvious that we only need the final parameters, the rest of the URL is\nalways the same. Knowing that, we can pass an array of those parameters on ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT")," and build the URLs dynamically, which would allow us to scrape\ndifferent categories without changing the source code. Let's get to it!"),(0,i.kt)("p",null,"First, we set up our ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT"),", either in the ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT")," form of the actor on the Apify platform, or by creating an ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT.json")," in our default key-value store\nlocally."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'["TRAVEL", "ECOMMERCE", "ENTERTAINMENT"]\n')),(0,i.kt)("p",null,"Once we have that, we can load it in the actor and populate the crawler's sources with it. In the following example, we're using the categories in the\ninput to construct the category URLs and we're also passing custom ",(0,i.kt)("inlineCode",{parentName:"p"},"userData")," to the sources. This means that the ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests")," that get created will\nautomatically contain this ",(0,i.kt)("inlineCode",{parentName:"p"},"userData"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// ...\nconst input = await Apify.getInput();\n\nconst sources = input.map(category => ({\n    url: `https://apify.com/store?category=${category}`,\n    userData: {\n        label: 'CATEGORY',\n    },\n}));\n\nconst requestList = await Apify.openRequestList('categories', sources);\n// ...\n")),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"userData.label")," is also a convention that we've been using for quite some time to label different ",(0,i.kt)("inlineCode",{parentName:"p"},"Requests"),". We know that this is a category URL\nso we ",(0,i.kt)("inlineCode",{parentName:"p"},"label")," it ",(0,i.kt)("inlineCode",{parentName:"p"},"CATEGORY"),". This way, we can easily make decisions in the ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," without having to inspect the URL itself."),(0,i.kt)("p",null,"We can then refactor the ",(0,i.kt)("inlineCode",{parentName:"p"},"if")," clauses in the ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction")," to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"label")," for decision-making. This does not make much sense for a crawler\nwith only two different pages, because a simple ",(0,i.kt)("inlineCode",{parentName:"p"},"boolean")," would suffice, but for pages with multiple different views, it becomes very useful."),(0,i.kt)("h4",{id:"structuring-the-code-better"},"Structuring the code better"),(0,i.kt)("p",null,"But perhaps we should not stop at refactoring the ",(0,i.kt)("inlineCode",{parentName:"p"},"if")," clauses. There are several ways we can make the actor look more elegant and - most\nimportantly - easier to reason about and make changes to."),(0,i.kt)("p",null,"In the following code we've made several changes."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Split the code into multiple files."),(0,i.kt)("li",{parentName:"ul"},"Added the ",(0,i.kt)("inlineCode",{parentName:"li"},"Apify.utils.log")," and replaced ",(0,i.kt)("inlineCode",{parentName:"li"},"console.log")," with it."),(0,i.kt)("li",{parentName:"ul"},"Added a ",(0,i.kt)("inlineCode",{parentName:"li"},"getSources()")," function to encapsulate ",(0,i.kt)("inlineCode",{parentName:"li"},"INPUT")," consumption."),(0,i.kt)("li",{parentName:"ul"},"Added a ",(0,i.kt)("inlineCode",{parentName:"li"},"createRouter()")," function to make our routing cleaner, without nested ",(0,i.kt)("inlineCode",{parentName:"li"},"if")," clauses."),(0,i.kt)("li",{parentName:"ul"},"Removed the ",(0,i.kt)("inlineCode",{parentName:"li"},"maxRequestsPerCrawl")," limit.")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"To create a multi-file actor on the Apify Platform, select ",(0,i.kt)("strong",{parentName:"p"},"Multiple source files")," in the ",(0,i.kt)("strong",{parentName:"p"},"Type")," dropdown on the ",(0,i.kt)("strong",{parentName:"p"},"Source")," screen.")),(0,i.kt)("p",null,"In our ",(0,i.kt)("inlineCode",{parentName:"p"},"main.js")," file, we place the general structure of the crawler:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// main.js\nconst Apify = require('apify');\nconst tools = require('./tools');\nconst {\n    utils: { log },\n} = Apify;\n\nApify.main(async () => {\n    log.info('Starting actor.');\n    const requestList = await Apify.openRequestList('categories', await tools.getSources());\n    const requestQueue = await Apify.openRequestQueue();\n    const router = tools.createRouter({ requestQueue });\n\n    log.debug('Setting up crawler.');\n    const crawler = new Apify.CheerioCrawler({\n        requestList,\n        requestQueue,\n        handlePageFunction: async context => {\n            const { request } = context;\n            log.info(`Processing ${request.url}`);\n            await router(request.userData.label, context);\n        },\n    });\n\n    log.info('Starting the crawl.');\n    await crawler.run();\n    log.info('Actor finished.');\n});\n")),(0,i.kt)("p",null,"Then in a separate ",(0,i.kt)("inlineCode",{parentName:"p"},"tools.js"),", we add our helper functions:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// tools.js\nconst Apify = require('apify');\nconst routes = require('./routes');\nconst {\n    utils: { log },\n} = Apify;\n\nexports.getSources = async () => {\n    log.debug('Getting sources.');\n    const input = await Apify.getInput();\n    return input.map(category => ({\n        url: `https://apify.com/store?category=${category}`,\n        userData: {\n            label: 'CATEGORY',\n        },\n    }));\n};\n\nexports.createRouter = globalContext => {\n    return async function(routeName, requestContext) {\n        const route = routes[routeName];\n        if (!route) throw new Error(`No route for name: ${routeName}`);\n        log.debug(`Invoking route: ${routeName}`);\n        return route(requestContext, globalContext);\n    };\n};\n")),(0,i.kt)("p",null,"And finally our routes in a separate ",(0,i.kt)("inlineCode",{parentName:"p"},"routes.js")," file:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"// routes.js\nconst Apify = require('apify');\nconst {\n    utils: { log },\n} = Apify;\n\nexports.CATEGORY = async ({ $, request }, { requestQueue }) => {\n    return Apify.utils.enqueueLinks({\n        $,\n        requestQueue,\n        selector: 'div.item > a',\n        baseUrl: request.loadedUrl,\n        transformRequestFunction: req => {\n            req.userData.label = 'DETAIL';\n            return req;\n        },\n    });\n};\n\nexports.DETAIL = async ({ $, request }) => {\n    const urlArr = request.url.split('/').slice(-2);\n\n    log.debug('Scraping results.');\n    const results = {\n        url: request.url,\n        uniqueIdentifier: urlArr.join('/'),\n        owner: urlArr[0],\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(',', ''),\n        ),\n    };\n\n    log.debug('Pushing data to dataset.');\n    await Apify.pushData(results);\n};\n")),(0,i.kt)("p",null,"Let us tell you a little bit more about the changes. We hope that in the end, you'll agree that this structure makes the actor more readable and\nmanageable."),(0,i.kt)("h4",{id:"splitting-your-code-into-multiple-files"},"Splitting your code into multiple files"),(0,i.kt)("p",null,"This was not always the case, but now that the Apify platform has a multifile editor, there's no reason not to split your code into multiple files and keep\nyour logic separate. Less code in a single file means less code you need to think about at any time, and that's a great thing!"),(0,i.kt)("h4",{id:"using-apifyutilslog-instead-of-consolelog"},"Using ",(0,i.kt)("inlineCode",{parentName:"h4"},"Apify.utils.log")," instead of ",(0,i.kt)("inlineCode",{parentName:"h4"},"console.log")),(0,i.kt)("p",null,"We wont go to great lengths here to talk about ",(0,i.kt)("inlineCode",{parentName:"p"},"utils.log"),", because you can read ",(0,i.kt)("a",{parentName:"p",href:"../api/log",target:null,rel:null},"it all in the documentation"),", but there's just\none thing that we need to stress: ",(0,i.kt)("strong",{parentName:"p"},"log levels"),"."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"utils.log")," enables you to use different log levels, such as ",(0,i.kt)("inlineCode",{parentName:"p"},"log.debug"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"log.info")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"log.warning"),". It not only makes your log more readable, but\nit also allows selective turning off of some levels by either calling the ",(0,i.kt)("inlineCode",{parentName:"p"},"utils.log.setLevel()")," function or by setting an ",(0,i.kt)("inlineCode",{parentName:"p"},"APIFY_LOG_LEVEL")," variable.\nThis is huge! Because you can now add a lot of debug logs in your actor, which will help you when something goes wrong and turn them on or off with a\nsimple ",(0,i.kt)("inlineCode",{parentName:"p"},"INPUT")," change, or by setting an environment variable."),(0,i.kt)("p",null,"The punch line? Use ",(0,i.kt)("inlineCode",{parentName:"p"},"Apify.utils.log")," instead of ",(0,i.kt)("inlineCode",{parentName:"p"},"console.log")," now and thank us later when something goes wrong!"),(0,i.kt)("h4",{id:"using-a-router-to-structure-your-crawling"},"Using a router to structure your crawling"),(0,i.kt)("p",null,"At first, it might seem more readable using just a simple ",(0,i.kt)("inlineCode",{parentName:"p"},"if / else")," statement to select different logic based on the crawled pages, but trust me, it\nbecomes far less impressive when working with more than two different pages and it definitely starts to fall apart when the logic to handle each page\nspans tens or hundreds of lines of code."),(0,i.kt)("p",null,"It's good practice in any programming to split your logic into bite-sized chunks that are easy to read and reason about. Scrolling through a\nthousand line long ",(0,i.kt)("inlineCode",{parentName:"p"},"handlePageFunction()")," where everything interacts with everything and variables can be used everywhere is not a beautiful thing to\ndo and a pain to debug. That's why we prefer the separation of routes into a special file and with large routes, we would even suggest having one file\nper route."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"TO BE CONTINUED with details on ",(0,i.kt)("inlineCode",{parentName:"p"},"PuppeteerCrawler")," and other features...")))}c.isMDXComponent=!0},12872:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/description-f9aec3abf3d17de2239e058d5dce838e.jpg"},56281:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/modified-date-5516a5ec24f9b0ef2f6a69f196e005db.jpg"},35631:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/scraping-practice-c0dd0ead6c920dcf687f0bb9d09087a9.jpg"},67543:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/title-01-424dd1253b7b093145561728d490d202.jpg"}}]);