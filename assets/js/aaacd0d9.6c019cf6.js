"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[3734],{68889:(e,t,r)=>{r.d(t,{Z:()=>i});var n=r(67294),a=r(88746),s=r(6141),o=r(6832);const i=function(e){var t=e.to,r=e.children,i=(0,s.E)(),u=i.version,l=i.isLast;if((0,o.default)().siteConfig.presets[0][1].docs.disableVersioning)return n.createElement(a.default,{to:"/api/"+t},r);var p=u+"/";return"current"===u?p="next/":l&&(p=""),n.createElement(a.default,{to:"/api/"+p+t},r)}},84446:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>d,contentTitle:()=>p,default:()=>y,frontMatter:()=>l,metadata:()=>c,toc:()=>h});var n,a=r(87462),s=r(63366),o=(r(67294),r(3905)),i=r(68889),u=["components"],l={id:"request-storage",title:"Request Storage"},p=void 0,c={unversionedId:"guides/request-storage",id:"version-3.1/guides/request-storage",title:"Request Storage",description:"The Apify SDK has several request storage types that are useful for specific tasks. The requests are stored either on local disk to a directory defined by the",source:"@site/versioned_docs/version-3.1/guides/request_storage.mdx",sourceDirName:"guides",slug:"/guides/request-storage",permalink:"/sdk/js/docs/guides/request-storage",draft:!1,tags:[],version:"3.1",lastUpdatedBy:"renovate[bot]",lastUpdatedAt:1694569557,formattedLastUpdatedAt:"Sep 13, 2023",frontMatter:{id:"request-storage",title:"Request Storage"},sidebar:"docs",previous:{title:"Apify Platform",permalink:"/sdk/js/docs/guides/apify-platform"},next:{title:"Result Storage",permalink:"/sdk/js/docs/guides/result-storage"}},d={},h=[{value:"Request queue",id:"request-queue",level:2},{value:"Request list",id:"request-list",level:2},{value:"Which one to choose?",id:"which-one-to-choose",level:2}],m=(n="CrawleeApiLink",function(e){return console.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)}),f={toc:h},q="wrapper";function y(e){var t=e.components,r=(0,s.Z)(e,u);return(0,o.kt)(q,(0,a.Z)({},f,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"The Apify SDK has several request storage types that are useful for specific tasks. The requests are stored either on local disk to a directory defined by the\n",(0,o.kt)("inlineCode",{parentName:"p"},"APIFY_LOCAL_STORAGE_DIR")," environment variable, or on the ",(0,o.kt)("a",{parentName:"p",href:"/docs/guides/apify-platform",target:null,rel:null},"Apify platform")," under the user account identified by the API token defined by the ",(0,o.kt)("inlineCode",{parentName:"p"},"APIFY_TOKEN")," environment variable. If neither of these variables is defined, by default Apify SDK sets ",(0,o.kt)("inlineCode",{parentName:"p"},"APIFY_LOCAL_STORAGE_DIR")," to ",(0,o.kt)("inlineCode",{parentName:"p"},"./storage")," in the current working directory and prints a warning."),(0,o.kt)("p",null,"Typically, you will be developing the code on your local computer and thus set the ",(0,o.kt)("inlineCode",{parentName:"p"},"APIFY_LOCAL_STORAGE_DIR")," environment variable. Once the code is ready, you will deploy it to the Apify platform, where it will automatically set the ",(0,o.kt)("inlineCode",{parentName:"p"},"APIFY_TOKEN")," environment variable and thus use cloud storage. No code changes are needed."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Related links")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.apify.com/storage",target:"_blank",rel:"noopener"},"Apify platform storage documentation")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://console.apify.com/storage",target:"_blank",rel:"noopener"},"View storage in Apify Console")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.apify.com/api/v2#/reference/request-queues",target:"_blank",rel:"noopener"},"Request queues API reference"))),(0,o.kt)("h2",{id:"request-queue"},"Request queue"),(0,o.kt)("p",null,"The request queue is a storage of URLs to crawl. The queue is used for the deep crawling of websites, where you start with several URLs and then recursively follow links to other pages. The data structure supports both breadth-first and depth-first crawling orders."),(0,o.kt)("p",null,"Each actor run is associated with a ",(0,o.kt)("strong",{parentName:"p"},"default request queue"),", which is created exclusively for the actor run. Typically, it is used to store URLs to crawl in the specific actor run. Its usage is optional."),(0,o.kt)("p",null,"In Apify SDK, the request queue is represented by the ",(0,o.kt)(i.Z,{to:"apify/class/RequestQueue",mdxType:"ApiLink"},(0,o.kt)("inlineCode",{parentName:"p"},"RequestQueue"))," class."),(0,o.kt)("p",null,"In local configuration, the request queue is emulated by ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/apify/apify-storage-local-js",target:"_blank",rel:"noopener"},"@apify/storage-local")," NPM package and its data is stored in SQLite database in the directory specified by the ",(0,o.kt)("inlineCode",{parentName:"p"},"APIFY_LOCAL_STORAGE_DIR")," environment variable as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"{APIFY_LOCAL_STORAGE_DIR}/request_queues/{QUEUE_ID}/db.sqlite\n")),(0,o.kt)("p",null,"Note that ",(0,o.kt)("inlineCode",{parentName:"p"},"{QUEUE_ID}")," is the name or ID of the request queue. The default queue has ID ",(0,o.kt)("inlineCode",{parentName:"p"},"default"),", unless you override it by setting the ",(0,o.kt)("inlineCode",{parentName:"p"},"APIFY_DEFAULT_REQUEST_QUEUE_ID")," environment variable."),(0,o.kt)("p",null,"The following code demonstrates basic operations of the request queue:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-javascript"},"// Open the default request queue associated with the actor run\nconst requestQueue = await RequestQueue.open();\n// Enqueue the initial request\nawait requestQueue.addRequest({ url: 'https://example.com' });\n\n// The crawler will automatically process requests from the queue\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    handlePageFunction: async ({ $, request }) => {\n        // Add new request to the queue\n        await requestQueue.addRequest({ url: 'https://example.com/new-page' });\n        // Add links found on page to the queue\n        await Actor.utils.enqueueLinks({ $, requestQueue });\n    },\n});\n")),(0,o.kt)("p",null,"To see more detailed example of how to use the request queue with a crawler, see the ",(0,o.kt)("a",{parentName:"p",href:"/docs/examples/puppeteer-crawler",target:null,rel:null},"Puppeteer Crawler")," example."),(0,o.kt)("h2",{id:"request-list"},"Request list"),(0,o.kt)("p",null,"The request list is not a storage per se - it represents the list of URLs to crawl that is stored in a run memory (or optionally in default ",(0,o.kt)("a",{parentName:"p",href:"../guides/result-storage#key-value-store",target:null,rel:null},"Key-Value Store")," associated with the run, if specified). The list is used for the crawling of a large number of URLs, when you know all the URLs which should be visited by the crawler and no URLs would be added during the run. The URLs can be provided either in code or parsed from a text file hosted on the web."),(0,o.kt)("p",null,"Request list is created exclusively for the actor run and only if its usage is explicitly specified in the code. Its usage is optional."),(0,o.kt)("p",null,"In Apify SDK, the request list is represented by the ",(0,o.kt)(m,{to:"core/class/RequestList",mdxType:"CrawleeApiLink"},(0,o.kt)("inlineCode",{parentName:"p"},"RequestList"))," class."),(0,o.kt)("p",null,"The following code demonstrates basic operations of the request list:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-javascript"},"// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    handlePageFunction: async ({ page, request }) => {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n")),(0,o.kt)("p",null,"To see more detailed example of how to use the request list with a crawler, see the ",(0,o.kt)("a",{parentName:"p",href:"/docs/examples/puppeteer-with-proxy",target:null,rel:null},"Puppeteer with proxy")," example."),(0,o.kt)("h2",{id:"which-one-to-choose"},"Which one to choose?"),(0,o.kt)("p",null,"When using Request queue - you would normally have several start URLs (e.g. category pages on e-commerce website) and then recursively add more (e.g. individual item pages) programmatically to the queue, it supports dynamic adding and removing of requests. No more URLs can be added to Request list after its initialization as it is immutable, URLs cannot be removed from the list either."),(0,o.kt)("p",null,"On the other hand, the Request queue is not optimized for adding or removing numerous URLs in a batch. This is technically possible, but requests are added one by one to the queue, and thus it would take significant time with a larger number of requests. Request list however can contain even millions of URLs, and it would take significantly less time to add them to the list, compared to the queue."),(0,o.kt)("p",null,"Note that Request queue and Request list can be used together by the same crawler.\nIn such cases, each request from the Request list is enqueued into the Request queue first (to the foremost position in the queue, even if Request queue is not empty) and then consumed from the latter.\nThis is necessary to avoid the same URL being processed more than once (from the list first and then possibly from the queue).\nIn practical terms, such a combination can be useful when there are numerous initial URLs, but more URLs would be added dynamically by the crawler."),(0,o.kt)("p",null,"The following code demonstrates how to use Request queue and Request list in the same crawler:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-javascript"},"// Prepare the sources array with URLs to visit (it can contain millions of URLs)\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n// Open the request list\nconst requestList = await RequestList.open('my-list', sources);\n\n// Open the default request queue. It's not necessary to add any requests to the queue\nconst requestQueue = await RequestQueue.open();\n\n// The crawler will automatically process requests from the list and the queue\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    // Each request from the request list is enqueued to the request queue one by one.\n    // At this point request with the same URL would exist in the list and the queue\n    handlePageFunction: async ({ request, page }) => {\n        // Add new request to the queue\n        await requestQueue.addRequest({ url: 'http://www.example.com/new-page' });\n\n        // Add links found on page to the queue\n        await Actor.utils.enqueueLinks({ page, requestQueue });\n\n        // The requests above would be added to the queue (but not to the list)\n        // and would be processed after the request list is empty.\n        // No more requests could be added to the list here\n    },\n});\n")))}y.isMDXComponent=!0},3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>m});var n=r(67294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function s(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?s(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):s(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},s=Object.keys(e);for(n=0;n<s.length;n++)r=s[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)r=s[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var u=n.createContext({}),l=function(e){var t=n.useContext(u),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},p=function(e){var t=l(e.components);return n.createElement(u.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,s=e.originalType,u=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),c=l(r),h=a,m=c["".concat(u,".").concat(h)]||c[h]||d[h]||s;return r?n.createElement(m,o(o({ref:t},p),{},{components:r})):n.createElement(m,o({ref:t},p))}));function m(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=r.length,o=new Array(s);o[0]=h;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i[c]="string"==typeof e?e:a,o[1]=i;for(var l=2;l<s;l++)o[l]=r[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}h.displayName="MDXCreateElement"}}]);